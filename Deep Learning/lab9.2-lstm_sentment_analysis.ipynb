{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-based sentiment classifier\n",
    "The code below illustrates a working example of a CNN-based sentiment classifier. The data used to train the model is from coursework assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and take a quick look\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "raw_data = pd.read_csv('../class8/coursework1_train.csv') # put the dataset from CW1 under the same directory\n",
    "raw_data = shuffle(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry num 40000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data and its class distribution\n",
    "\n",
    "# only use a small fraction of the data to speed up training and testing\n",
    "# training on the full dataset requires a large RAM (>30GB) or a GPU card\n",
    "all_text = raw_data['text'].tolist()[:40000] \n",
    "all_raw_labels = raw_data['sentiment'].tolist()[:40000]\n",
    "labels_list = ['pos','neg']\n",
    "all_labels = [labels_list.index(ll) for ll in all_raw_labels]\n",
    "\n",
    "print('entry num', len(all_text))\n",
    "print(len([ll  for ll in  all_labels if ll==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg in train 17484\n",
      "neg in dev 2516\n"
     ]
    }
   ],
   "source": [
    "# data split. \n",
    "# Feel free to use differnt raios to split the data.\n",
    "train_docs = all_text[:35000]\n",
    "train_labels = all_labels[:35000]\n",
    "dev_docs = all_text[35000:]\n",
    "dev_labels = all_labels[35000:]\n",
    "\n",
    "print('neg in train', len([ll for ll in train_labels if ll==1]))\n",
    "print('neg in dev', len([ll for ll in dev_labels if ll==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now I love Bela Lugosi,don\\'t get me wrong,he is one of the most interesting people to ever make a movie but he certainly did his share of clunkers.This is just another one of those.<br /><br />Lugosi plays Dr.Lorenz,a doctor who has had his medical license pulled for unexplained reasons.He is however doing experiments to keep his wife young and beautiful.It\\'s revealed that she is 70-80 years old yet Lugosi looks to be in his mid 50\\'s so why he is married to this old woman is never really explained.<br /><br />Anyway these treatments or experiments involved giving brides who are at the altar being married some sort of sweet smelling substance whereby they pass out but are thought to be dead.Then Lugosi and some of his assistants steal the body on its way to the morgue and take it back to his lab where it\\'s kept in some sort of suspended animation or catatonic state.Then the stolen brides have a needle rammed somewhere in their bodies,maybe the neck,and then the needle is rammed into the body of Lugosi\\'s wife to bring her back to youth and beauty.We never really see where Lugosi sticks the needle or what it is that he draws out of the brides but it somehow restores his wife .Apparently old age makes you scream with pain because Lugosi\\'s wife does a lot of screaming until she gets back to her younger state.Helping Lugosi in his lab is the only good thing about this movie....a weird old hag and her two deformed sons....one son is a big lumpy looking slow acting fellow who likes to fondle the snoozing brides and the other son is a mean little dwarf....little person, to be politically correct in today\\'s world.At night these three just sort of pile up and sleep in Lugosi\\'s dreary downstairs lab.Who these 3 are and how they came to be Lugosi\\'s scared assistants is,like a lot of stuff in this film, never explained.<br /><br />So anyway a female reporter is given the assignment by her gruff editor to find out where all the stolen brides are going to.She quickly figures out that the one common thing among all the stolen brides is a rare orchid that is found on them.So she asks around and is told that there is a world renowned orchid expert living nearby who just happens to be the one who developed this particular orchid.This expert turns out to be creepy Dr.Lorenz.She quickly tracks him down and upsets his little house of horrors.I\\'m not sure where the police were during all this but they came in to mop up after the reporter had done all the dirty work.<br /><br />It seems that Lugosi\\'s movies always had some sort of unnecessary silly plot line that just made the whole thing stink to high heavens.I mean a world famous orchid expert kidnaps brides by sending them a doped up orchid he himself is known to have developed? D\\'OH!<br /><br />And then later it\\'s revealed that the young ladies don\\'t even have to be brides for the procedure to work so why would Lugosi keep kidnapping brides from heavily guarded churches for his experiments and create all the attention and newspaper headlines? Why not just grab a prostitute off the street like a normal weirdo pervert would do? This clunker reminded me a lot of another Lugosi stinker,\"The Devil Bat\"....same silly plot lines and bad acting and same silly \\'reporter gets bad guy\\' deal.<br /><br />But Lugosi is always good--he is creepy and sinister enough to keep you interested at least enough to keep watching him.The woman playing the reporter was just a terrible actor....she had no emotion whatsoever,she just delivered her lines like a machine gun ,spewing them out as quickly as she could.Everyone else pretty much blew too,when it came to being good actors.<br /><br />But this thing is watchable ,if only for Bela Lugosi fans.Lugosi was always so intense even when the picture was a dog.He must have known he was doing terrible pictures but maybe he also knew that if he gave it everything he had a little of that intensity might shine through past all the bad plots and bad acting which surrounded him.<br /><br />And he was right----we horror fans will always have a love for Bela Lugosi.He gave it his all every time he was in front of the camera.We do give two f**ks for you,Bela.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_docs[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained glove embeddings\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# specify the loaction of the downloaded glove file\n",
    "path_of_downloaded_files = \"/home/cim/staff/uhac002/Library/Embeddings/GloVe/glove.6B.300d.txt\"\n",
    "# path_of_downloaded_files = \"/Users/yg211/Embeddings/Glove/glove.6B.300d.txt\"\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we define the RNN-based classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, model_type, cls_num, pooler_type, dropout, gpu):\n",
    "        super(RNN_Classifier, self).__init__()\n",
    "        assert model_type in ['rnn','lstm','bilstm','gru']\n",
    "        assert pooler_type in ['max','avg']\n",
    "        # rnn type\n",
    "        if model_type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'bilstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, bidirectional=True, dropout=dropout)\n",
    "        else: # model_type == 'gru'\n",
    "            self.rnn = nn.GRU(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        # map from rnn output to logits\n",
    "        if model_type == 'bilstm':\n",
    "            self.fc = nn.Linear(2*hidden_dim, cls_num)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, cls_num)\n",
    "        # pooler type\n",
    "        self.pooler_type = pooler_type\n",
    "        # gpu or not\n",
    "        self.gpu = gpu\n",
    "        if gpu: self.to('cuda')\n",
    "            \n",
    "    def forward(self, input_matrix):\n",
    "        token_num = input_matrix.shape[1]\n",
    "        hidden_vecs = self.rnn(input_matrix)[0]\n",
    "        if self.pooler_type == 'max':\n",
    "            pooler = nn.MaxPool1d(token_num)\n",
    "        else: \n",
    "            pooler = nn.AvgPool1d(token_num)\n",
    "        if self.gpu: pooler.to('cuda')\n",
    "        pooled_hidden = pooler(torch.transpose(hidden_vecs,1,2)).squeeze()\n",
    "        return self.fc(pooled_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0008, -0.0544],\n",
      "        [ 0.0256, -0.0171],\n",
      "        [ 0.0205, -0.0428]], device='cuda:0', grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "# define functions that build mini-batches\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "embd_dim = 300\n",
    "hidden_dim = 300\n",
    "rnn_type = 'bilstm'\n",
    "pooler_type = 'avg'\n",
    "dropout = 0.5\n",
    "gpu = True\n",
    "\n",
    "oov_vec = oov_vec = np.random.rand(embd_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    return np.array(vecs)\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors):\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def make_batch_prediction(sent_list, word_vectors, model, use_gpu=False):\n",
    "    batch = build_mini_batch(sent_list, word_vectors)\n",
    "    batch_logits = torch.tensor([])\n",
    "    if use_gpu: batch_logits = batch_logits.to('cuda')\n",
    "    for i in range(batch.shape[0]):\n",
    "        input_sents = torch.from_numpy(batch[i]).float()\n",
    "        if use_gpu: input_sents = input_sents.to('cuda')\n",
    "        logits = model(input_sents.unsqueeze(0))\n",
    "        batch_logits = torch.cat( (batch_logits, logits) )\n",
    "    return batch_logits.view(batch.shape[0],-1)\n",
    "  \n",
    "# sanity check \n",
    "model = RNN_Classifier(embd_dim, hidden_dim, rnn_type, len(labels_list), pooler_type, dropout, gpu)\n",
    "batch_pred = make_batch_prediction(\n",
    "    ['hello world!','hello','another test sentence this is'],\n",
    "    word_vectors, model, gpu)\n",
    "print(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 20 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 50\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.999) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======epoch 0 loss====== 0.40404513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 1/20 [13:57<4:25:21, 837.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-F1 on dev set is 0.8705735709787179\n",
      "learning rate 0.001\n",
      "best model updated; new best macro-F1 0.8705735709787179\n",
      "\n",
      "======epoch 1 loss====== 0.27735937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 2/20 [27:56<4:11:24, 838.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 1 the macro-F1 on dev set is 0.8987578670739615\n",
      "learning rate 0.000999\n",
      "best model updated; new best macro-F1 0.8987578670739615\n",
      "\n",
      "======epoch 2 loss====== 0.24083634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 3/20 [41:53<3:57:21, 837.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 2 the macro-F1 on dev set is 0.9089950165671072\n",
      "learning rate 0.000998001\n",
      "best model updated; new best macro-F1 0.9089950165671072\n",
      "\n",
      "======epoch 3 loss====== 0.20332158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 4/20 [55:52<3:43:29, 838.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 3 the macro-F1 on dev set is 0.9081835134316718\n",
      "learning rate 0.000997002999\n",
      "\n",
      "======epoch 4 loss====== 0.1676065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 5/20 [1:09:48<3:29:26, 837.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 4 the macro-F1 on dev set is 0.9077976949423736\n",
      "learning rate 0.000996005996001\n",
      "\n",
      "======epoch 5 loss====== 0.1414379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 6/20 [1:23:47<3:15:31, 837.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 5 the macro-F1 on dev set is 0.9091886086190651\n",
      "learning rate 0.000995009990004999\n",
      "best model updated; new best macro-F1 0.9091886086190651\n",
      "\n",
      "======epoch 6 loss====== 0.10959197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 7/20 [1:37:45<3:01:32, 837.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 6 the macro-F1 on dev set is 0.8900502542567806\n",
      "learning rate 0.000994014980014994\n",
      "\n",
      "======epoch 7 loss====== 0.08169704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 8/20 [1:51:43<2:47:36, 838.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 7 the macro-F1 on dev set is 0.9023969391680123\n",
      "learning rate 0.000993020965034979\n",
      "\n",
      "======epoch 8 loss====== 0.0692254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 9/20 [2:05:40<2:33:35, 837.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 8 the macro-F1 on dev set is 0.8982878888425979\n",
      "learning rate 0.000992027944069944\n",
      "\n",
      "======epoch 9 loss====== 0.04858781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 10/20 [2:19:39<2:19:39, 837.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 9 the macro-F1 on dev set is 0.8984325803281097\n",
      "learning rate 0.000991035916125874\n",
      "\n",
      "======epoch 10 loss====== 0.038732626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11/20 [2:33:36<2:05:39, 837.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 10 the macro-F1 on dev set is 0.8900746914388564\n",
      "learning rate 0.0009900448802097482\n",
      "\n",
      "======epoch 11 loss====== 0.030272696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 12/20 [2:47:35<1:51:45, 838.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 11 the macro-F1 on dev set is 0.8995189560091588\n",
      "learning rate 0.0009890548353295385\n",
      "\n",
      "======epoch 12 loss====== 0.025217999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13/20 [3:01:31<1:37:43, 837.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 12 the macro-F1 on dev set is 0.9053983312265628\n",
      "learning rate 0.000988065780494209\n",
      "\n",
      "======epoch 13 loss====== 0.01648644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 14/20 [3:15:29<1:23:45, 837.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 13 the macro-F1 on dev set is 0.9063898751288939\n",
      "learning rate 0.0009870777147137147\n",
      "\n",
      "======epoch 14 loss====== 0.012434055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15/20 [3:29:27<1:09:48, 837.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 14 the macro-F1 on dev set is 0.9029313180834231\n",
      "learning rate 0.000986090636999001\n",
      "\n",
      "======epoch 15 loss====== 0.0150765665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 16/20 [3:43:25<55:50, 837.75s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 15 the macro-F1 on dev set is 0.9028727205278886\n",
      "learning rate 0.000985104546362002\n",
      "\n",
      "======epoch 16 loss====== 0.015444373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 17/20 [3:57:23<41:53, 837.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 16 the macro-F1 on dev set is 0.8999000441881245\n",
      "learning rate 0.00098411944181564\n",
      "\n",
      "======epoch 17 loss====== 0.011175384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 18/20 [4:11:20<27:55, 837.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 17 the macro-F1 on dev set is 0.9002589901499143\n",
      "learning rate 0.0009831353223738245\n",
      "\n",
      "======epoch 18 loss====== 0.010966059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19/20 [4:25:17<13:57, 837.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 18 the macro-F1 on dev set is 0.9095779949008391\n",
      "learning rate 0.0009821521870514505\n",
      "best model updated; new best macro-F1 0.9095779949008391\n",
      "\n",
      "======epoch 19 loss====== 0.008545001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 20/20 [4:39:14<00:00, 837.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 19 the macro-F1 on dev set is 0.8949937476105054\n",
      "learning rate 0.000981170034864399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training the CNN model\n",
    "\n",
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    for idx in range(0,len(train_docs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        sents = train_docs[idx:idx+batch_size]\n",
    "        if len(sents) == 0: break\n",
    "        y_target = torch.tensor([train_labels[idx:idx+batch_size]], dtype=torch.int64).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = make_batch_prediction(sents, word_vectors, model, gpu)\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 4+: clip the gradient, to avoid gradient explosion\n",
    "        nn.utils.clip_grad_value_(model.parameters(), clip_value=3.)\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        test_docs = dev_docs\n",
    "        test_labels = dev_labels\n",
    "        \n",
    "        for idx in range(0,len(test_docs),batch_size):\n",
    "            y_pred = make_batch_prediction(\n",
    "                test_docs[idx:idx+batch_size], word_vectors, model, gpu)\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "            predictions += pred_labels\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(test_labels, predictions,average='macro')\n",
    "        print('\\n---> after epoch {} the macro-F1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best macro-F1',f1)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "* Optimize the hyper parameters, e.g. using different RNN architectures, dropout rates and hidden state dimensions.\n",
    "* (Optional) You may have noticed that the current implementation sends one sentence to the RNN model at a time (see function *make_batch_prediction*). To speed up the training we may want to let the model make predictions for multiple sentences at once. Consider to how to implement the batch prediction. *Hint*: you may need to zero-pad the shorter sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
