{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This problem is taken from Kaggle and is known as NLP with Disaster Tweets.\n",
    "link to this problem : https://www.kaggle.com/c/nlp-getting-started/overview\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing the required libraries first."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets import the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "train_data = pd.read_csv(\"/Users/rohankilledar/Documents/projects/Kaggle/NLP_with_disaster_tweets/train.csv\")\n",
    "\n",
    "#the label of these test_set are unknown as part of the compitition\n",
    "test_data = pd.read_csv(\"/Users/rohankilledar/Documents/projects/Kaggle/NLP_with_disaster_tweets/test.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets have a look at the data we have at hand."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "print(train_data[train_data[\"target\"]==0][\"text\"].values[1])\n",
    "print(train_data[train_data[\"target\"]==1][\"text\"].values[1])\n",
    "print(len(train_data))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I love fruits\n",
      "Forest fire near La Ronge Sask. Canada\n",
      "7613\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#lowercasing all the words and making a list of those words\n",
    "all_text = [words.lower() for words in train_data[\"text\"]]\n",
    "tokens = [word_tokenize(word) for word in all_text]\n",
    "\n",
    "#removing the stopwords from the list of words\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "clean_tokens = []\n",
    "\n",
    "for tweet in tokens:\n",
    "    token_wo_sw = [word for word in tweet if word not in en_stopwords and word not in string.punctuation]\n",
    "    clean_tokens.extend(token_wo_sw)\n",
    "\n",
    "clean_tokens\n",
    "\n",
    "word_freq = FreqDist(clean_tokens)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "dictFD = dict(word_freq)\n",
    "print(\"total number of unique tokens in the training set are: \"+ str(len(dictFD)))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total number of unique tokens in the training set are: 22891\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to split down our training set into train set and validation set.\n",
    "I'll split them in ratio of 3:1\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "def divide_ratio(data,ratio):\n",
    "    leng = len(data)\n",
    "    splitter = int(leng*ratio)\n",
    "    return data[:splitter],data[splitter:]\n",
    "\n",
    "ratio = 0.75\n",
    "train_text,validation_text = divide_ratio(all_text,ratio)\n",
    "train_label, validation_label = divide_ratio(train_data[\"target\"].tolist(),ratio)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use TF-IDF to vectorize the text and take the max_feature_num as 1000"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "max_feature_number = 1000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_number)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "validation_vecs = TfidfVectorizer(max_features=max_feature_number,vocabulary=train_vectorizer.vocabulary_).fit_transform(validation_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_number,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_data[\"text\"].tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets try using Logistic regression to figure out the label: 0 or 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#train model\n",
    "clf =  LogisticRegression().fit(train_vecs, train_label)\n",
    "#test model\n",
    "test_pred = clf.predict(validation_vecs)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "\n",
    "acc = accuracy_score(validation_label, test_pred)\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(validation_label, test_pred, average='macro')\n",
    "print('acc', acc)\n",
    "print('precision', pre)\n",
    "print('rec', rec)\n",
    "print('f1', f1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc 0.7547268907563025\n",
      "precision 0.7612253148326893\n",
      "rec 0.7433568662058145\n",
      "f1 0.7456856965937189\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "adding the prediction to sample submission file for test.csv dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "sample_submission = pd.read_csv(\"/Users/rohankilledar/Documents/projects/Kaggle/NLP_with_disaster_tweets/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(test_vecs)\n",
    "sample_submission.head"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          id  target\n",
       "0         0       1\n",
       "1         2       0\n",
       "2         3       1\n",
       "3         9       0\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "344443636c3027c5042750c9c609acdda283a9c43681b128a8c1053e7ad2aa7d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}