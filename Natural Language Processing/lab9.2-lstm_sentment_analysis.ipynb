{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-based sentiment classifier\n",
    "The code below illustrates a working example of a CNN-based sentiment classifier. The data used to train the model is from coursework assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and take a quick look\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "raw_data = pd.read_csv('coursework1_train.csv') # put the dataset from CW1 under the same directory\n",
    "raw_data = shuffle(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry num 40000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data and its class distribution\n",
    "\n",
    "# only use a small fraction of the data to speed up training and testing\n",
    "# training on the full dataset requires a large RAM (>30GB) or a GPU card\n",
    "all_text = raw_data['text'].tolist()[:40000] \n",
    "all_raw_labels = raw_data['sentiment'].tolist()[:40000]\n",
    "labels_list = ['pos','neg']\n",
    "all_labels = [labels_list.index(ll) for ll in all_raw_labels]\n",
    "\n",
    "print('entry num', len(all_text))\n",
    "print(len([ll  for ll in  all_labels if ll==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg in train 17432\n",
      "neg in dev 2568\n"
     ]
    }
   ],
   "source": [
    "# data split. \n",
    "# Feel free to use differnt raios to split the data.\n",
    "train_docs = all_text[:35000]\n",
    "train_labels = all_labels[:35000]\n",
    "dev_docs = all_text[35000:]\n",
    "dev_labels = all_labels[35000:]\n",
    "\n",
    "print('neg in train', len([ll for ll in train_labels if ll==1]))\n",
    "print('neg in dev', len([ll for ll in dev_labels if ll==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A previous IMDb reviewer has stated that 'Rafter Romance' is a 'rip-off' (that's the other reviewer's term) of a German musical called 'Me By Day, You By Night'. Apparently that reviewer is unaware that *both* of these films have borrowed their premise from 'Box and Cox', an English play written by John Maddison Morton in 1847. This play deals with two tradesmen who rent the same room from an unscrupulous landlady, each man believing himself the sole tenant. Because the two men have different work schedules, the ruse is not discovered straight away. This play was once so popular in Britain that 'to Box and Cox it' became a common term for an arrangement in which two people willingly shared accommodations meant for only one person.<br /><br />The innovation of 'Rafter Romance' (and its predecessor) is that the two tenants are now a man and a woman, who inevitably develop a romance. As is usual in these cornball movies, the guy and the gal detest each other until they fall into each other's arms. Hoo boy.<br /><br />The landlord in this film is played by George Sidney, a character actor who specialised in playing Jewish stereotypes that were meant to be sympathetic. George Sidney was never as annoying as the odious Harry Green (the Jewish equivalent of Stepin Fetchit) but Sidney's depictions of Jewish characters are still exaggerated and embarrassing to watch.<br /><br />The single most notable thing about 'Rafter Romance' is that, to my knowledge, this is the earliest Hollywood film to make reference to Hitler and the rise of Nazism. At one point in this movie, landlord Eckbaum (Sidney) discovers his teenage son Julius engaged in chalking swastikas on the walls. Eckbaum and his son are clearly meant to be Jewish. Admittedly, nobody in Hollywood in 1933 had any real idea of what Hitler was planning for the Jews in Europe ... still, it's surprising to see a film depicting a Jewish teenager who thinks that swastikas are a joke. His father is, quite properly, angered by this display of the Nazi symbol.<br /><br />A very shameful aspect of Hollywood history is the documented fact that all of the major Hollywood studios continued to do business with the Third Reich as late as 1939. Hollywood's leading ladies were medically documented as 'Aryan' so that their films could be distributed in Nazi Germany and Austria. For the same reason, Hollywood's leading men were documented as 'Aryan and uncircumcised'. Except for Darryl Zanuck at Twentieth Century-Fox, all the Hollywood studio executives who colluded in this policy were Jewish ... but clearly had no objection to doing business with Hitler. I'm surprised that 'Rafter Romance' contains a scene depicting swastikas unfavourably, as this sequence would have rendered the film Verboten in Germany and Austria. (Maybe the scene was cut out for German release: it isn't crucial to the movie's plot.) Apart from this, the movie contains nothing notable. Robert Benchley does his usual unfunny befuddled characterisation: I've never understood the appeal of this man. I'll rate 'Rafter Romance' 4 out of 10.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_docs[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-b3c78221b7d3>:12: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained glove embeddings\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# specify the loaction of the downloaded glove file\n",
    "path_of_downloaded_files = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/Natural Language Processing/glove.6B.300d.txt\"\n",
    "# path_of_downloaded_files = \"/Users/yg211/Embeddings/Glove/glove.6B.300d.txt\"\n",
    "#path_of_downloaded_files = 'glove.6B.300d.txt'\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we define the RNN-based classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, model_type, cls_num, pooler_type, dropout, gpu=False):\n",
    "        super(RNN_Classifier, self).__init__()\n",
    "        assert model_type in ['rnn','lstm','bilstm','gru']\n",
    "        assert pooler_type in ['max','avg']\n",
    "        # rnn type\n",
    "        if model_type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'bilstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, bidirectional=True, dropout=dropout)\n",
    "        else: # model_type == 'gru'\n",
    "            self.rnn = nn.GRU(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        # map from rnn output to logits\n",
    "        if model_type == 'bilstm':\n",
    "            self.fc = nn.Linear(2*hidden_dim, cls_num)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, cls_num)\n",
    "        # pooler type\n",
    "        self.pooler_type = pooler_type\n",
    "        # gpu or not\n",
    "        self.gpu = gpu\n",
    "        if gpu: self.to('cuda')\n",
    "            \n",
    "    def forward(self, input_matrix):\n",
    "        token_num = input_matrix.shape[1]\n",
    "        hidden_vecs = self.rnn(input_matrix)[0]\n",
    "        if self.pooler_type == 'max':\n",
    "            pooler = nn.MaxPool1d(token_num)\n",
    "        else: \n",
    "            pooler = nn.AvgPool1d(token_num)\n",
    "        if self.gpu: pooler.to('cuda')\n",
    "        pooled_hidden = pooler(torch.transpose(hidden_vecs,1,2)).squeeze()\n",
    "        return self.fc(pooled_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0141,  0.0129],\n",
      "        [-0.0364,  0.0285],\n",
      "        [ 0.0002,  0.0403]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "<ipython-input-21-85d351cb3641>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(text_vecs)\n"
     ]
    }
   ],
   "source": [
    "# define functions that build mini-batches\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "embd_dim = 300\n",
    "hidden_dim = 300\n",
    "rnn_type = 'bilstm'\n",
    "pooler_type = 'avg'\n",
    "dropout = 0.5\n",
    "gpu = False\n",
    "\n",
    "oov_vec = oov_vec = np.random.rand(embd_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    return np.array(vecs)\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors):\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def make_batch_prediction(sent_list, word_vectors, model, use_gpu=False):\n",
    "    batch = build_mini_batch(sent_list, word_vectors)\n",
    "    batch_logits = torch.tensor([])\n",
    "    if use_gpu: batch_logits = batch_logits.to('cuda')\n",
    "    for i in range(batch.shape[0]):\n",
    "        input_sents = torch.from_numpy(batch[i]).float()\n",
    "        if use_gpu: input_sents = input_sents.to('cuda')\n",
    "        logits = model(input_sents.unsqueeze(0))\n",
    "        batch_logits = torch.cat( (batch_logits, logits) )\n",
    "    return batch_logits.view(batch.shape[0],-1)\n",
    "  \n",
    "# sanity check \n",
    "model = RNN_Classifier(embd_dim, hidden_dim, rnn_type, len(labels_list), pooler_type, dropout, gpu)\n",
    "batch_pred = make_batch_prediction(\n",
    "    ['hello world!','hello','another test sentence this is'],\n",
    "    word_vectors, model, gpu)\n",
    "print(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 20 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 50\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.999) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]<ipython-input-21-85d351cb3641>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(text_vecs)\n"
     ]
    }
   ],
   "source": [
    "# training the CNN model\n",
    "\n",
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    for idx in range(0,len(train_docs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        sents = train_docs[idx:idx+batch_size]\n",
    "        if len(sents) == 0: break\n",
    "        y_target = torch.tensor([train_labels[idx:idx+batch_size]], dtype=torch.int64).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = make_batch_prediction(sents, word_vectors, model, gpu)\n",
    "        pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 4+: clip the gradient, to avoid gradient explosion\n",
    "        nn.utils.clip_grad_value_(model.parameters(), clip_value=3.)\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        test_docs = dev_docs\n",
    "        test_labels = dev_labels\n",
    "        \n",
    "        for idx in range(0,len(test_docs),batch_size):\n",
    "            y_pred = make_batch_prediction(\n",
    "                test_docs[idx:idx+batch_size], word_vectors, model, gpu)\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "            predictions += pred_labels\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(test_labels, predictions,average='macro')\n",
    "        print('\\n---> after epoch {} the macro-F1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best macro-F1',f1)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "* Optimize the hyper parameters, e.g. using different RNN architectures, dropout rates and hidden state dimensions.\n",
    "* (Optional) You may have noticed that the current implementation sends one sentence to the RNN model at a time (see function *make_batch_prediction*). To speed up the training we may want to let the model make predictions for multiple sentences at once. Consider to how to implement the batch prediction. *Hint*: you may need to zero-pad the shorter sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
