{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_pickle('unprocessed_dataset.pkl')\n",
    "type(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase): \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase) \n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)  \n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)  \n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)  \n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)  \n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)  \n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)  \n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)  \n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)  \n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)  \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92579/92579 [00:36<00:00, 2533.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "article_text = []\n",
    "stories = df['story'].tolist()\n",
    "for i in tqdm(stories):\n",
    "    dash_indx = i.find('(CNN) --')\n",
    "    if dash_indx>=0: #and dash_indx<=20:\n",
    "        i = i[dash_indx+len('(CNN) --'):]\n",
    "    tt = re.sub(r'\\n',' ', i)\n",
    "    tt=re.sub(r\"([?!¿])\", r\" \\1 \", tt)\n",
    "    tt=decontracted(tt)\n",
    "    tt = re.sub('[^A-Za-z0-9.,]+', ' ', tt)\n",
    "    tt = tt.lower()\n",
    "    article_text.append(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['story']= article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output_file = open('cleaned_df.pkl','wb')\n",
    "pickle.dump(df, output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def RepresentsInt(s):\n",
    "#     try: \n",
    "#         int(s)\n",
    "#         return True\n",
    "#     except ValueError:\n",
    "#         return False\n",
    "\n",
    "# data = dataset[191690]['story']\n",
    "# extra_signs = '£$@'\n",
    "# table = str.maketrans('', '', string.punctuation + extra_signs)\n",
    "# # from nltk.stem import WordNetLemmatizer\n",
    "# # wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# import inflect\n",
    "# p = inflect.engine()\n",
    "# import unicodedata\n",
    "\n",
    "# #new_str = unicodedata.normalize(\"NFKD\", unicode_str)\n",
    "\n",
    "# # custom_punctuation = \"!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "# # table = str.maketrans('', '', custom_punctuation)\n",
    "\n",
    "# def clean_data(data):\n",
    "#     data = unicodedata.normalize(\"NFKD\", data)\n",
    "#     data = sent_tokenize(data)\n",
    "#     # data = data.split('. ')\n",
    "#     #considering lematization\n",
    "#     cleaned = list()\n",
    "#     for sentence in data:\n",
    "#         if len(sentence)>0:\n",
    "            \n",
    "#             dash_indx = sentence.find('--')\n",
    "#             if dash_indx>=0:\n",
    "#                 sentence = sentence[dash_indx+len('--'):]\n",
    "#             # sentence = sentence.strip()\n",
    "#             sentence = sentence.replace('-',' ')\n",
    "#             word = word_tokenize(sentence)\n",
    "#             #word = [w.lower() for w in word]\n",
    "#             word = [w.translate(table).lower() for w in word if len(w)>0]\n",
    "#            # print(word)\n",
    "#             #word = [wordnet_lemmatizer.lemmatize(w) for w in word]\n",
    "#             word = [p.number_to_words(w) if RepresentsInt(w) and int(w)<99999 else w for w in word]\n",
    "#             cleaned.append(' '.join(word))\n",
    "        \n",
    "# \t# remove empty strings\n",
    "#     # cleaned = [c for c in cleaned if len(c) > 0]\n",
    "    \n",
    "\n",
    "#     data = '. '.join(cleaned)\n",
    "    \n",
    "#     return re.sub(' +', ' ', data)\n",
    "# print(type(data))\n",
    "# clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dataset[53734] # this one contains a long number with more than 100 characters denoting the value of pi and is causing issue when converting numbers to their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(dataset))):\n",
    "#     dataset[i]['story'] = clean_data(dataset[i]['story'])\n",
    "#     dataset[i]['highlight'] = clean_data(dataset[i]['highlight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('cleaned_df.pkl')\n",
    "article_count = 0\n",
    "summary_count = 0\n",
    "dataset_size = len(df)\n",
    "\n",
    "article = df['story'].tolist()\n",
    "summary = df['highlight'].tolist()\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    article_count += len(article[i].split())\n",
    "    summary_count += len(summary[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Article token count:  668\n",
      "Average Summary token count:  41\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "print('Average Article token count: ', floor(article_count/dataset_size))\n",
    "print('Average Summary token count: ', floor(summary_count/dataset_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the word_count for the articles in our dataset to understand the length of these articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_pd.drop('hightlight1', axis=1, inplace=True)\n",
    "df['word_count_text'] = df['story'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check for 0-100 percentile range in steps of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 percentile value is 0\n",
      " 10 percentile value is 259\n",
      " 20 percentile value is 348\n",
      " 30 percentile value is 432\n",
      " 40 percentile value is 520\n",
      " 50 percentile value is 617\n",
      " 60 percentile value is 719\n",
      " 70 percentile value is 829\n",
      " 80 percentile value is 957\n",
      " 90 percentile value is 1156\n",
      "100 percentile value is  1946\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(0,100,10) :\n",
    "    var =df[\"word_count_text\"]. values\n",
    "    var = np.sort(var,axis = None)\n",
    "    print (\" {} percentile value is {}\".format (i,var[int(len(var)*(float(i)/100))]))\n",
    "print (\"100 percentile value is \",var[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking the values in between 90 and 100. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 percentile value is 1156\n",
      "91 percentile value is 1183\n",
      "92 percentile value is 1216\n",
      "93 percentile value is 1248\n",
      "94 percentile value is 1288\n",
      "95 percentile value is 1334\n",
      "96 percentile value is 1384\n",
      "97 percentile value is 1447\n",
      "98 percentile value is 1530\n",
      "99 percentile value is 1640\n",
      "100 percentile value is  1946\n"
     ]
    }
   ],
   "source": [
    "for i in range (90,100) :\n",
    "    var =df[\"word_count_text\"].values\n",
    "    var = np.sort(var, axis = None)\n",
    "    print(\"{} percentile value is {}\".format(i, var[int (len(var)*(float (i)/100))]))\n",
    "print (\"100 percentile value is \", var[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume the lenght of articles in the range of 1200-1300 as 95% articles are less than that."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
