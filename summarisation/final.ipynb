{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "from os import listdir\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_file_name = 'unprocessed_file.pkl'\n",
    "cleaned_file_name ='cleaned_file.pkl'\n",
    "cnn = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/summarisation/cnn/stories\"\n",
    "path_of_downloaded_files = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/Natural Language Processing/glove.6B.50d.txt\"\n",
    "filename = \"glove.6B.50d.txt\"\n",
    "embedding_dim = 50\n",
    "data_size = 45000\n",
    "hidden_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-dd21ec6755cf>:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 400000 word vectors from glove.6B.50d.txt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(filename)\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "print('loaded %s word vectors from %s.' % (len(word_vectors.key_to_index),filename ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "sos = 'sos'\n",
    "eos = 'eos'\n",
    "\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.words = []\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0  # Count SOS and EOS\n",
    "        self.glove = {}\n",
    "        self.embedding_dim = word_vectors.get_vector('office').shape[0]\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in word_tokenize(sentence):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            self.words.append(word)\n",
    "            if word not in self.glove:\n",
    "                self.glove[word] = np.random.normal(scale=0.6 , size= (self.embedding_dim,))\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def addPretrained(self,path_of_downloaded_files):\n",
    "        with open(path_of_downloaded_files) as f:\n",
    "            for indx,l in enumerate(f):\n",
    "                line = l.split()\n",
    "                word = line[0]\n",
    "                if word == sos:\n",
    "                    first_word = self.words[0]\n",
    "                    self.words.append(first_word)\n",
    "                    self.words[0] = sos\n",
    "                    self.word2index[first_word] = indx\n",
    "                    self.word2index[sos] = 0\n",
    "                    self.index2word[indx] = first_word\n",
    "                    self.index2word[0] = sos\n",
    "                    self.glove[sos] = word_vectors[indx]\n",
    "                    self.glove[first_word] = word_vectors[0]\n",
    "                    self.word2count[sos] = 1\n",
    "                    self.n_words += 1\n",
    "\n",
    "                elif word == eos:\n",
    "                    sec_word = self.words[1]\n",
    "                    self.words.append(sec_word)\n",
    "                    self.words[1] = eos\n",
    "                    self.index2word[indx] = sec_word\n",
    "                    self.index2word[1] = eos\n",
    "                    self.word2index[sec_word] = indx\n",
    "                    self.word2index[eos] = 1\n",
    "                    self.glove[eos] = word_vectors[indx]\n",
    "                    self.glove[sec_word] = word_vectors[1]\n",
    "                    self.word2count[eos] = 1\n",
    "                    self.n_words += 1\n",
    "                else:\n",
    "                    self.words.append(word)\n",
    "                    self.word2count[word] = 1\n",
    "                    self.word2index[word] = indx\n",
    "                    self.index2word[indx]=word\n",
    "                    self.glove[word] = word_vectors[indx]\n",
    "                    self.n_words += 1\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def decontracted(phrase): \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase) \n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)  \n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)  \n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)  \n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)  \n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)  \n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)  \n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)  \n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)  \n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)  \n",
    "    return phrase\n",
    "\n",
    "\n",
    "# %%\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    \n",
    "    \n",
    "    dash_indx = s.find('(CNN) --')\n",
    "    if dash_indx>=0: #and dash_indx<=20:\n",
    "        s = s[dash_indx+len('(CNN) --'):]\n",
    "        \n",
    "    s=decontracted(s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdaae229c064f378e841a7585a1880c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "def read_file(filename):\n",
    "    file = open(filename, encoding= 'UTF-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def split_text(article):\n",
    "    indx = article.index('@highlight')\n",
    "    story = article[:indx]\n",
    "    highlight = article[indx:].split('@highlight')\n",
    "\n",
    "    highlight = \". \".join([h.strip() for h in highlight if len(h)>0])\n",
    "    return story,highlight\n",
    "\n",
    "\n",
    "# %%\n",
    "def read_all(folder):\n",
    "    dataset = list()\n",
    "\n",
    "    for indx, file in tqdm(enumerate(listdir(folder))):\n",
    "        if indx == data_size:\n",
    "            break\n",
    "        filename = folder + '/' + file\n",
    "        article = read_file(filename)\n",
    "        story,highlight = split_text(article)\n",
    "\n",
    "        dataset.append({'story':story, 'highlight':highlight})\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = read_all(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "#saving dataset for cleaning\n",
    "import pickle\n",
    "output_file = open(unprocessed_file_name,'wb')\n",
    "pickle.dump(dataset, output_file)\n",
    "output_file.close()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "dataset = pd.read_pickle(unprocessed_file_name)\n",
    "#reducing the dataset for initial testing of model\n",
    "#dataset = dataset[:data_size]\n",
    "\n",
    "\n",
    "# %%\n",
    "df = pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean word count length of text article is 696\n",
      "The mean word count length of summary/highlight is 46\n",
      "The max word count length of text article is 1968\n",
      "The max word count length of summary/highlight is 96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "df['story'] = df['story'].apply(lambda x: normalizeString(x))\n",
    "df['highlight'] = df['highlight'].apply(lambda x: normalizeString(x))\n",
    "\n",
    "\n",
    "# %%\n",
    "df['highlight'] = df['highlight'].apply(lambda x: sos + \" \" + x )\n",
    "\n",
    "\n",
    "# %%\n",
    "df['word_count_text'] = df['story'].apply(lambda x: len(str(x).split()))\n",
    "df['highlight_count'] = df['highlight'].apply(lambda x: len(str(x).split()))\n",
    "from math import floor\n",
    "print(\"The mean word count length of text article is \" + str(floor(df['word_count_text'].mean())))\n",
    "print(\"The mean word count length of summary/highlight is \" + str(floor(df['highlight_count'].mean())))\n",
    "\n",
    "max_article_len = floor(df['word_count_text'].max())\n",
    "max_summary_len = floor(df['highlight_count'].max())\n",
    "print(\"The max word count length of text article is \" + str(max_article_len))\n",
    "print(\"The max word count length of summary/highlight is \" + str(max_summary_len))\n",
    "\n",
    "\n",
    "# %%\n",
    "df.drop('word_count_text', axis=1, inplace=True)\n",
    "df.drop('highlight_count', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "with open(\"processed_data.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(df, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a95c05c3454b7a914ec9e688cc0e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted Words:\n",
      "vocab 445120\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "vocab = Lang('vocab')\n",
    "vocab.addPretrained(path_of_downloaded_files)\n",
    "def prepareData(vocab):\n",
    "    for indx,row in tqdm(df.iterrows()):\n",
    "        vocab.addSentence(row['story'])\n",
    "        vocab.addSentence(row['highlight'])\n",
    "    print(\"counted Words:\")\n",
    "    print(vocab.name, vocab.n_words)\n",
    "prepareData(vocab)\n",
    "\n",
    "\n",
    "# %%\n",
    "with open(\"vocab.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(df, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "pairs = df.values.tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    # return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    return [lang.word2index[word] for word in word_tokenize(sentence)]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(vocab, pair[0])\n",
    "    target_tensor = tensorFromSentence(vocab, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "def generate_weights(unique_word_corpus,glove, tens=True):\n",
    "    matrix_len = len(unique_word_corpus)\n",
    "    unique_word2indx = {}\n",
    "    \n",
    "    weight_matrix  = np.zeros((matrix_len, embedding_dim))\n",
    "   \n",
    "    words_found = 0\n",
    "\n",
    "    for indx, word in enumerate(unique_word_corpus):\n",
    "        try:\n",
    "            unique_word2indx[word] = indx\n",
    "            weight_matrix[indx] = glove[word]\n",
    "            words_found +=1\n",
    "        except KeyError:\n",
    "            weight_matrix[indx] = np.random.normal(scale=0.6 , size= (embedding_dim,))\n",
    "            unique_word2indx[word] = indx\n",
    "    if tens:\n",
    "        return torch.from_numpy(weight_matrix) , unique_word2indx\n",
    "    else:\n",
    "        return weight_matrix,unique_word2indx\n",
    "\n",
    "weight_matrix, meh= generate_weights(vocab.words, vocab.glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = 1\n",
    "\n",
    "       # self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab.n_words,embedding_dim= embedding_dim)\n",
    "        self.embedding = self.embedding.from_pretrained(weight_matrix)\n",
    "        #self.embedding.from_pretrained(glove)\n",
    "        self.lstm = nn.LSTM(input_size = hidden_dim, hidden_size = hidden_dim) #, num_layers = 1, bidirectional = True, batch_first = True)\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = embedded.float()\n",
    "       # print(output)\n",
    "        #print(hidden)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #in case of LSTM we need h0 and c0 hence init this as a tuple (h0,c0) and passed as hidden to lstm and in case of gru its just h0\n",
    "       return (torch.zeros(self.num_layer, 1, self.hidden_size, dtype=torch.float, device=device), torch.zeros(self.num_layer,1,self.hidden_size,dtype = torch.float, device= device))\n",
    "\n",
    "\n",
    "# %%\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings= output_size,embedding_dim= hidden_size)\n",
    "        self.embedding.from_pretrained(weight_matrix)\n",
    "        self.lstm = nn.LSTM(input_size= hidden_size, hidden_size= hidden_size) #, num_layers = 1, bidirectional = False, batch_first = True)\n",
    "        #self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.fc = nn.Linear(in_features= hidden_size , out_features=vocab.n_words)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = F.relu(embedded)\n",
    "        \n",
    "        #hidden is [2,1,50]\n",
    "        #emb is [1,1,50]\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        #output = self.softmax(self.out(output[0]))\n",
    "        output = self.fc(output[0])\n",
    "        \n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "# %%\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=max_article_len):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.embedding.from_pretrained(weight_matrix)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "       \n",
    "    \n",
    "        AttnDecoderRNN.embedded = embedded\n",
    "        \n",
    "        AttnDecoderRNN.hidden = hidden\n",
    "        AttnDecoderRNN.encoder_outputs = encoder_outputs\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0].view(1,-1)), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "                                 \n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "       return (torch.zeros(1, 1, self.hidden_size, dtype=torch.int64, device=device), torch.zeros(1,1,self.hidden_size,dtype = torch.int64, device= device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, attn, max_length=vocab.n_words):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            if attn :\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden= decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                #tar_tens = weight_matrix[target_tensor[di].item()].view(-1).float()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            \n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            if attn :\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden= decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            tar_tens = weight_matrix[target_tensor[di].item()].view(-1).float()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def trainIters(encoder, decoder, n_iters, attn=False, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    if attn:\n",
    "        print(\"training with attention.\")\n",
    "    print(datetime.now())\n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    print('encoder optimizer init')\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    print('decoder optimizer init')\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print('choosing training pairs:' + str(n_iters))\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    \n",
    "    print('init criterion')\n",
    "    criterion = nn.NLLLoss()\n",
    "    # criterion = nn.MSELoss()\n",
    "    \n",
    "    print('starting iterations')\n",
    "    \n",
    "    for iter in tqdm(range(1, n_iters + 1)):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, attn)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def evaluate(encoder, decoder, sentence ,max_length=max_summary_len, attn=False): # please check what should be max_length\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(vocab, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "        print('evaluation init')\n",
    "        for ei in range(input_length):\n",
    "            \n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            \n",
    "            if attn:\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden) \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('sos')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638162305.20294\n",
      "encoder optimizer init\n",
      "decoder optimizer init\n",
      "choosing training pairs:35000\n",
      "init criterion\n",
      "starting iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf12d30bd344598bc284326235886f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 19s (- 1031m 11s) (1000 2%) 9.0173\n",
      "59m 43s (- 985m 31s) (2000 5%) 8.0657\n",
      "87m 1s (- 928m 11s) (3000 8%) 7.8709\n",
      "113m 34s (- 880m 8s) (4000 11%) 7.7212\n",
      "139m 58s (- 839m 52s) (5000 14%) 7.5641\n",
      "166m 29s (- 804m 43s) (6000 17%) 7.6307\n",
      "192m 44s (- 770m 56s) (7000 20%) 7.5351\n",
      "218m 41s (- 738m 6s) (8000 22%) 7.4875\n",
      "243m 17s (- 702m 50s) (9000 25%) 7.1365\n",
      "268m 11s (- 670m 27s) (10000 28%) 7.0072\n",
      "292m 31s (- 638m 13s) (11000 31%) 6.9060\n",
      "316m 53s (- 607m 23s) (12000 34%) 6.8298\n",
      "341m 12s (- 577m 25s) (13000 37%) 6.8442\n",
      "365m 48s (- 548m 43s) (14000 40%) 6.9311\n",
      "391m 13s (- 521m 37s) (15000 42%) 7.0102\n",
      "417m 18s (- 495m 32s) (16000 45%) 6.8895\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "hidden_size = 50\n",
    "voc_size = len(vocab.words)\n",
    "hidden_dim = 50\n",
    "# max_summary_size = 60\n",
    "encoder1 = EncoderRNN(vocab.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, vocab.n_words).to(device)\n",
    "#attn_decoder1 = AttnDecoderRNN(hidden_size, vocab.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "\n",
    "# %%\n",
    "trainIters(encoder1, decoder1, 35000, print_every=250)\n",
    "\n",
    "# trainIters(encoder1, attn_decoder1,1000,True, print_every=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "import pickle\n",
    "\n",
    "# save model and other necessary modules\n",
    "all_info_want_to_save = {\n",
    "    'encoder': encoder1,\n",
    "    'decoder': decoder1,\n",
    "    'vocab': vocab\n",
    "}\n",
    "\n",
    "with open(\"train_model.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(all_info_want_to_save, save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "import pickle\n",
    "\n",
    "# load the saved file\n",
    "with open('train_model.pkl','rb') as ff:\n",
    "    saved_info = pickle.load(ff)\n",
    "    \n",
    "# extract the information from the saved file\n",
    "encoder1 = saved_info['encoder']\n",
    "decoder1 = saved_info['decoder']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        #print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0],attn=False)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "evaluateRandomly(encoder1, decoder1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
