{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "from pprint import pprint\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_downloaded_files = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/Natural Language Processing/glove.6B.50d.txt\"\n",
    "filename = \"glove.6B.50d.txt\"\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "sos = 'sos'\n",
    "eos = 'eos'\n",
    "\n",
    "embedding_dim = hidden_size = 50\n",
    "data_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_file = datapath(path_of_downloaded_files)\n",
    "# word2vec_glove_file = get_tmpfile(filename)\n",
    "# glove2word2vec(glove_file, word2vec_glove_file)\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "# print('loaded %s word vectors from %s.' % (len(word_vectors.key_to_index),filename ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/rohankilledar/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n",
      "Reusing dataset cnn_dailymail (/Users/rohankilledar/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n",
      "Reusing dataset cnn_dailymail (/Users/rohankilledar/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n",
      "Reusing dataset cnn_dailymail (/Users/rohankilledar/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n",
      "100%|██████████| 3/3 [00:00<00:00, 41.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287113\n",
      "11490\n",
      "13368\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('ccdv/cnn_dailymail','3.0.0',split = 'train')  \n",
    "test_dataset = load_dataset('ccdv/cnn_dailymail','3.0.0',split = 'test')  \n",
    "validation_dataset = load_dataset('ccdv/cnn_dailymail','3.0.0',split = 'validation')  \n",
    "\n",
    "entire_dataset = load_dataset('ccdv/cnn_dailymail', '3.0.0')\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(test_dataset.__len__())\n",
    "print(validation_dataset.__len__())\n",
    "print(entire_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embedding_glove = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train' : train_dataset,\n",
    "    'test' : test_dataset,\n",
    "    'validation' : validation_dataset\n",
    "}\n",
    "\n",
    "with open(\"hf_dataset.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(dataset, save_path)\n",
    "\n",
    "# with open('hf_dataset.pkl','rb') as ff:\n",
    "#     hfd = pickle.load(ff)\n",
    "\n",
    "# train_ds = hfd['train']\n",
    "# train_df = train_ds.data.to_pandas()\n",
    "# test_ds = hfd['test']\n",
    "# test_df = test_ds.data.to_pandas()\n",
    "# valid_ds = hfd['validation']\n",
    "# valid_df = valid_ds.data.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_dataset.data.to_pandas()\n",
    "# train_df = pd.DataFrame.from_dict(train_dataset)\n",
    "train_df = train_dataset.data.to_pandas()\n",
    "test_df = test_dataset.data.to_pandas()\n",
    "valid_df = validation_dataset.data.to_pandas()\n",
    "dataset_list = [train_df,test_df,valid_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's official: U.S. President Barack Obama wan...</td>\n",
       "      <td>Syrian official: Obama climbed to the top of t...</td>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Usain Bolt rounded off the world cham...</td>\n",
       "      <td>Usain Bolt wins third gold of world championsh...</td>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kansas City, Missouri (CNN) -- The General Ser...</td>\n",
       "      <td>The employee in agency's Kansas City office is...</td>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Los Angeles (CNN) -- A medical doctor in Vanco...</td>\n",
       "      <td>NEW: A Canadian doctor says she was part of a ...</td>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN) -- Police arrested another teen Thursday...</td>\n",
       "      <td>Another arrest made in gang rape outside Calif...</td>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  It's official: U.S. President Barack Obama wan...   \n",
       "1  (CNN) -- Usain Bolt rounded off the world cham...   \n",
       "2  Kansas City, Missouri (CNN) -- The General Ser...   \n",
       "3  Los Angeles (CNN) -- A medical doctor in Vanco...   \n",
       "4  (CNN) -- Police arrested another teen Thursday...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Syrian official: Obama climbed to the top of t...   \n",
       "1  Usain Bolt wins third gold of world championsh...   \n",
       "2  The employee in agency's Kansas City office is...   \n",
       "3  NEW: A Canadian doctor says she was part of a ...   \n",
       "4  Another arrest made in gang rape outside Calif...   \n",
       "\n",
       "                                         id  \n",
       "0  0001d1afc246a7964130f43ae940af6bc6c57f01  \n",
       "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef  \n",
       "2  00027e965c8264c35cc1bc55556db388da82b07f  \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a  \n",
       "4  0003ad6ef0c37534f80b55b4235108024b407f0b  "
      ]
     },
     "execution_count": 1006,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_reduced = train_dataset[:2000]\n",
    "# train_reduced = pd.DataFrame.from_dict(train_reduced)\n",
    "# train_reduced['article'] = train_reduced['article'].apply(lambda x: normalizeString(x))\n",
    "# train_reduced['highlights'] = train_reduced['highlights'].apply(lambda x: normalizeString(x))\n",
    "# train_reduced.drop('id', axis=1, inplace=True)\n",
    "# train_reduced.head()\n",
    "\n",
    "# train_reduced = train_reduced[train_reduced['article'].str.split().str.len().lt(mean_article_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase): \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase) \n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)  \n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)  \n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)  \n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)  \n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)  \n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)  \n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)  \n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)  \n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)  \n",
    "    phrase = re.sub(r'ain\\'t', \"is not\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    un = '(CNN)'\n",
    "    dash_indx = s.find(un)\n",
    "    if dash_indx>-1: #and dash_indx<=20:\n",
    "        s = s[dash_indx+len(un):]\n",
    "    \n",
    "    s = s.lower().strip()\n",
    "    s=decontracted(s)\n",
    "\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r'\\.', r' . ', s)\n",
    "    \n",
    "\n",
    "    # s = ' '.join([word for word in s.split(' ') if word not in stop_words and word in vocab.word2index.keys()])\n",
    "  \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def drop_data_id(dataset_list):\n",
    "    for data in dataset_list:\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "drop_data_id(dataset_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {'sos': 0 , 'eos': 1}\n",
    "        self.word2count = {'sos': 1, 'eos': 1}\n",
    "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def addFromPretrained(self,emb):\n",
    "        for i in range(len(embedding_glove)):\n",
    "            word = embedding_glove.itos[i]\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words +=1\n",
    "            \n",
    "\n",
    "# vocab = Lang('vocab')\n",
    "# vocab.addFromPretrained(embedding_glove)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['article'] = train_df['article'].apply(lambda x: normalizeString(x))\n",
    "train_df['highlights'] = train_df['highlights'].apply(lambda x: normalizeString(x))\n",
    "# test_df['article'] = test_df['article'].apply(lambda x: normalizeString(x))\n",
    "# test_df['highlights'] = test_df['highlights'].apply(lambda x: normalizeString(x))\n",
    "# valid_df['article'] = valid_df['article'].apply(lambda x: normalizeString(x))\n",
    "# valid_df['highlights'] = valid_df['highlights'].apply(lambda x: normalizeString(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean word count length of text article is 727\n",
      "The mean word count length of summary/highlight is 52\n",
      "The max word count length of text article is 2785\n",
      "The max word count length of summary/highlight is 1278\n"
     ]
    }
   ],
   "source": [
    "train_df['word_count_text'] = train_df['article'].apply(lambda x: len(str(x).split()))\n",
    "train_df['highlight_count'] = train_df['highlights'].apply(lambda x: len(str(x).split()))\n",
    "from math import floor\n",
    "\n",
    "mean_article_len = floor(train_df['word_count_text'].mean())\n",
    "mean_hightlight_len = floor(train_df['highlight_count'].mean())\n",
    "\n",
    "max_article_len = floor(train_df['word_count_text'].max())\n",
    "max_hightlight_len = floor(train_df['highlight_count'].max())\n",
    "\n",
    "\n",
    "\n",
    "print(\"The mean word count length of text article is \" + str(mean_article_len))\n",
    "print(\"The mean word count length of summary/highlight is \" + str(mean_hightlight_len))\n",
    "\n",
    "\n",
    "print(\"The max word count length of text article is \" + str(max_article_len))\n",
    "print(\"The max word count length of summary/highlight is \" + str(max_hightlight_len))\n",
    "\n",
    "train_df.drop('word_count_text', axis=1, inplace=True)\n",
    "train_df.drop('highlight_count', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"processed_train_df.DataFrame\",\"wb\") as save_path:\n",
    "    pickle.dump(train_df, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using this tokenizer as its the fastest way to tokenize sentences.\n",
    "WORD = re.compile(r'\\w+')\n",
    "def regTokenize(text):\n",
    "    words = WORD.findall(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Lang(object):\n",
    "#     def __init__(self,vocab_file) -> None:\n",
    "#         self.word2index = {}\n",
    "#         self.index2word = {}\n",
    "\n",
    "#         index = 0\n",
    "#         for word in [pad,unk,sos, eos]:\n",
    "#             self.word2index[word] = index\n",
    "#             self.index2word[index] = word\n",
    "#             index +=1\n",
    "\n",
    "#         with open(vocab_file, 'r') as f:\n",
    "#             for line in f:\n",
    "#                 try:\n",
    "#                     word = line.split()[0]\n",
    "#                     self.word2index[word] = index\n",
    "#                     self.index2word[index] = word\n",
    "#                     index +=1\n",
    "\n",
    "#                 except ValueError as e:\n",
    "#                     print(\"Error in line {}: {}\".format(token_counter - 4, e))\n",
    "\n",
    "#     def word2index(self,word: str) -> int:\n",
    "#         if word == eos:\n",
    "#             return self.word2index['.']\n",
    "#         elif word not in self.word2index:\n",
    "#             return self.word2index[]\n",
    "\n",
    "# new_vocab = Lang(path_of_downloaded_files)\n",
    "# print(len(new_vocab.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Lang:\n",
    "#     def __init__(self, name):\n",
    "#         self.name = name\n",
    "#         self.words = []\n",
    "#         self.word2index = {}\n",
    "#         self.word2count = {}\n",
    "#         self.index2word = {}\n",
    "#         self.n_words = 0  # Count SOS and EOS\n",
    "#         self.glove = {}\n",
    "#         self.embedding_dim = word_vectors.get_vector('office').shape[0]\n",
    "\n",
    "#     def addSentence(self, sentence):\n",
    "#         for word in regTokenize(sentence):\n",
    "#             self.addWord(word)\n",
    "\n",
    "#     def addWord(self, word):\n",
    "#         if word not in self.words:\n",
    "#             self.words.append(word)\n",
    "#             self.word2index[word] = self.n_words\n",
    "#             self.word2count[word] = 1\n",
    "#             self.index2word[self.n_words] = word\n",
    "#             self.n_words += 1\n",
    "#             if word not in self.glove:\n",
    "#                 self.glove[word] = np.random.normal(scale=0.6 , size= (self.embedding_dim,))\n",
    "#         else:\n",
    "#             self.word2count[word] += 1\n",
    "\n",
    "#     def addPretrained(self,path_of_downloaded_files):\n",
    "#         with open(path_of_downloaded_files) as f:\n",
    "#             for indx,l in tqdm(enumerate(f)):\n",
    "#                 line = l.split()\n",
    "#                 word = line[0]\n",
    "#                 if word == sos:\n",
    "#                     first_word = self.words[0]\n",
    "#                     self.words.append(first_word)\n",
    "#                     self.words[0] = sos\n",
    "#                     self.word2index[first_word] = indx\n",
    "#                     self.word2index[sos] = 0\n",
    "#                     self.index2word[indx] = first_word\n",
    "#                     self.index2word[0] = sos\n",
    "#                     self.glove[sos] = word_vectors[indx]\n",
    "#                     #self.glove[first_word] = word_vectors[0]\n",
    "#                     self.word2count[sos] = 1\n",
    "#                     self.n_words += 1\n",
    "\n",
    "#                 elif word == eos:\n",
    "#                     sec_word = self.words[1]\n",
    "#                     self.words.append(sec_word)\n",
    "#                     self.words[1] = eos\n",
    "#                     self.index2word[indx] = sec_word\n",
    "#                     self.index2word[1] = eos\n",
    "#                     self.word2index[sec_word] = indx\n",
    "#                     self.word2index[eos] = 1\n",
    "#                     self.glove[eos] = word_vectors[indx]\n",
    "#                     #self.glove[sec_word] = word_vectors[1]\n",
    "#                     self.word2count[eos] = 1\n",
    "#                     self.n_words += 1\n",
    "#                 else:\n",
    "#                     self.words.append(word)\n",
    "#                     self.word2count[word] = 1\n",
    "#                     self.word2index[word] = indx\n",
    "#                     self.index2word[indx]=word\n",
    "#                     self.glove[word] = word_vectors[indx]\n",
    "#                     self.n_words += 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d659bc7e5149a38d0d4197bce77005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted Words:\n",
      "vocab 658536\n"
     ]
    }
   ],
   "source": [
    "vocab = Lang('vocab')\n",
    "vocab.addFromPretrained(path_of_downloaded_files)\n",
    "def prepareData(vocab):\n",
    "    for _ ,row in tqdm(train_df.iterrows()):\n",
    "        vocab.addSentence(row['article'])\n",
    "        vocab.addSentence(row['highlights'])\n",
    "    print(\"counted Words:\")\n",
    "    print(vocab.name, vocab.n_words)\n",
    "    \n",
    "prepareData(vocab)\n",
    "\n",
    "with open(\"vocab.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(vocab, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(vocab, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = train_df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pair(data, max_len):\n",
    "    return len(data[0].split(' ')) < max_len\n",
    "\n",
    "def filter_pairs(pairs, max_len):\n",
    "    return [pair for pair in pairs if filter_pair(pair,max_len)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = filter_pairs(pairs, mean_article_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentences(lang, sentence):\n",
    "    return [lang.word2index[word] for word in regTokenize(sentence)]\n",
    "\n",
    "def tensorFromSentences(lang, sentence):\n",
    "    indexes = indexesFromSentences(lang,sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long).view(-1,1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromSentences(vocab, pair[0])\n",
    "    target_tensor = tensorFromSentences(vocab, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words processed: 658536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_weights(unique_word_corpus, tens=True):\n",
    "    matrix_len = len(unique_word_corpus)\n",
    "    \n",
    "    \n",
    "    weight_matrix  = np.zeros((matrix_len, embedding_dim))\n",
    "   \n",
    "    words_found = 0\n",
    "    \n",
    "\n",
    "    for indx, word in enumerate(unique_word_corpus):\n",
    "        # try:\n",
    "            # if word is not in the embedding this function assign zero value to that vector\n",
    "        weight_matrix[indx] = embedding_glove.get_vecs_by_tokens(word)\n",
    "        words_found +=1\n",
    "\n",
    "        # except KeyError:\n",
    "\n",
    "        #     weight_matrix[indx] = np.random.normal(scale=0.6 , size= (embedding_dim,))\n",
    "        #     random_weights +=1\n",
    "    print(f'total words processed: {words_found}')\n",
    "    # print(f'embedding initialized to random value for {random_weights} no. of words')\n",
    "    if tens:\n",
    "        return torch.from_numpy(weight_matrix) \n",
    "    else:\n",
    "        return weight_matrix\n",
    "\n",
    "weight_matrix= generate_weights(vocab.word2index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = weight_matrix.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "       \n",
    "\n",
    "       # self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        self.embedding = self.embedding.from_pretrained(weight_matrix, freeze = True)\n",
    "        \n",
    "        ## trying gru\n",
    "        # self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = hidden_size, hidden_size = hidden_size) #, num_layers = 1, bidirectional = True, batch_first = True)\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = embedded\n",
    "        \n",
    "        # output, hidden = self.gru(output, hidden)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        #in case of LSTM we need h0 and c0 hence init this as a tuple (h0,c0) and passed as hidden to lstm and in case of gru its just h0\n",
    "        return (torch.zeros(1, 1, self.hidden_size, dtype=torch.float, device=device), torch.zeros(1,1,self.hidden_size,dtype = torch.float, device= device))\n",
    "        # return torch.zeros(1,1,self.hidden_size, device=device)\n",
    "\n",
    "# %%\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings= output_size,embedding_dim= hidden_size)\n",
    "        # self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.embedding = self.embedding.from_pretrained(weight_matrix, freeze = True)\n",
    "        self.lstm = nn.LSTM(input_size= hidden_size, hidden_size= hidden_size) #, num_layers = 1, bidirectional = False, batch_first = True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)\n",
    "        # self.softmax = nn.Softmax(dim = 1)\n",
    "       # self.fc = nn.Linear(in_features= hidden_size , out_features=vocab.n_words)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = torch.tanh(embedded)\n",
    "        # output, hidden = self.gru(output, hidden)\n",
    "        output, hidden = self.lstm(output.float(), hidden)\n",
    "        #output = self.softmax(self.out(output[0]))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        # output = self.out(output[0])\n",
    "        return output, hidden\n",
    "\n",
    "    def initHiddden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, dtype=torch.float, device=device), torch.zeros(1,1,self.hidden_size,dtype = torch.float, device= device))\n",
    "\n",
    "\n",
    "# %%\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=999):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.embedding.from_pretrained(weight_matrix)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "       \n",
    "    \n",
    "        AttnDecoderRNN.embedded = embedded\n",
    "        \n",
    "        AttnDecoderRNN.hidden = hidden\n",
    "        AttnDecoderRNN.encoder_outputs = encoder_outputs\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0].view(1,-1)), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "                                 \n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "       return (torch.zeros(1, 1, self.hidden_size, dtype=torch.int64, device=device), torch.zeros(1,1,self.hidden_size,dtype = torch.int64, device= device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, attn, max_length=max_article_len):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            if attn :\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden= decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                #tar_tens = weight_matrix[target_tensor[di].item()].view(-1).float()\n",
    "                # target_tensor -> tensor(4,dtype=long)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            \n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            if attn :\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden= decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "            \n",
    "            # sm = nn.Softmax(decoder_output)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            # tar -> tensor((4)) dtype = long/int64\n",
    "            #dec -> size(1,vocab_size) dtype = float\n",
    "            # tar_tens = weight_matrix[target_tensor[di].item()].view(-1).float()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # sys.stdout.write(\"\\rtarget: %s\" % vocab.index2word[target_tensor[di].item()])\n",
    "            sys.stdout.write(\"\\rdecoded: %s\" % vocab.index2word[topi.item()])\n",
    "            sys.stdout.flush()\n",
    "            train.dout = decoder_output\n",
    "            # print(f'target output: {summary_vocab.index2word[target_tensor[di].item()]}')\n",
    "            # print(f'decoded output: {vocab.index2word[decoder_output.data.topk(1)[1].item()]} ')\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "#try different learning rate\n",
    "def trainIters(encoder, decoder, n_iters, attn=False, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    if attn:\n",
    "        print(\"training with attention.\")\n",
    "    \n",
    "\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    print('encoder optimizer init')\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    print('decoder optimizer init')\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    print('choosing training pairs:' + str(n_iters))\n",
    "    training_pairs = [tensorFromPair(random.choice(pairs))\n",
    "                      for _ in range(n_iters)]\n",
    "    \n",
    "    print(f'filtering the pairs with max article size of {mean_article_len}')\n",
    "    \n",
    "    \n",
    "    print('init criterion')\n",
    "    criterion = nn.NLLLoss()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    print('starting iterations')\n",
    "    \n",
    "    for iter in tqdm(range(1, n_iters + 1)):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, attn)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def evaluate(encoder, decoder, sentence ,max_length=mean_hightlight_len, attn=False): # please check what should be max_length\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentences(vocab, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_article_len, encoder.hidden_size, device=device)\n",
    "        print('evaluation init')\n",
    "        for ei in range(input_length):\n",
    "            \n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            \n",
    "            if attn:\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden) \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append(eos)\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder optimizer init\n",
      "decoder optimizer init\n",
      "choosing training pairs:75000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1153-1dd4755cf7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# trainIters(encoder1, attn_decoder1,1000,True, print_every=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1150-887de1e833f9>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, attn, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'choosing training pairs:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     training_pairs = [tensorFromPair(random.choice(pairs))\n\u001b[0m\u001b[1;32m     21\u001b[0m                       for _ in range(n_iters)]\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1150-887de1e833f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'choosing training pairs:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     training_pairs = [tensorFromPair(random.choice(pairs))\n\u001b[0m\u001b[1;32m     21\u001b[0m                       for _ in range(n_iters)]\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1143-4cf552927efc>\u001b[0m in \u001b[0;36mtensorFromPair\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1143-4cf552927efc>\u001b[0m in \u001b[0;36mtensorFromSentences\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1143-4cf552927efc>\u001b[0m in \u001b[0;36mindexesFromSentences\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregTokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1143-4cf552927efc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregTokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fref'"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "hidden_size = 50\n",
    "voc_size = len(vocab.word2index)\n",
    "hidden_dim = 50\n",
    "# max_summary_size = 60\n",
    "encoder1 = EncoderRNN(vocab.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, vocab.n_words).to(device)\n",
    "#attn_decoder1 = AttnDecoderRNN(hidden_size, vocab.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "\n",
    "# %%\n",
    "trainIters(encoder1, decoder1, 75000, print_every=2000)\n",
    "\n",
    "# trainIters(encoder1, attn_decoder1,1000,True, print_every=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1154-61fe6f052b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'fref'"
     ]
    }
   ],
   "source": [
    "vocab.word2index['fref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> by  .  graham smith  .  published  .  est september  .   .  updated  .  est september  .  a car filled with explosives rammed into a u  . s  .  government vehicle in the north west pakistani city of peshawar today killing three people and wounding others  .  two americans and two pakistanis working at the u  . s  .  consulate in peshawar were among the wounded according to a u  . s  .  embassy official speaking on condition of anonymity  .  two of the dead were pakistanis  .  the u  . s  .  embassy in islamabad had earlier denied initial reports that two americans were killed in the blast  .  scroll down for video  .  wreckage a car filled with explosives rammed into a u  . s  .  government vehicle in the north west pakistani city of peshawar today killing three and wounding  .  peshawar is located near pakistan is semi autonomous tribal region the main sanctuary for taliban and al qaida militants in the country  .  the city has been hit by scores of bombings in recent years but attacks against american targets are relatively rare likely because of the extensive security measures taken by the u  . s  .  government  .  the vehicle was attacked after it left the u  . s  .  consulate in peshawar and was travelling through an area of the city that hosts various international organisations including the united nations said police officer pervez khan who was part of the security escort for the vehicle as it moved  .  local tv footage showed an suv at the site that was completely destroyed and burned  .  all that was left was a carcass of blackened twisted metal  .  officer khan said the images were of the u  . s  .  vehicle that was attacked  .  a police explosives expert abdul haq said lbs of explosives were used in the attack  .  investigation the vehicle was attacked after it left the u  . s  .  consulate in peshawar and was travelling through an area of the city that hosts various international organisations including the un  .  security operation the u  . s  .  embassy in islamabad has denied initial reports that two americans were killed  .  irfan khan a local resident was at a nearby shop when the blast occurred  .  he said i quickly looked back in panic to see smoke and dust erupt from the scene  .  i ran toward the scene along with others and saw two vehicles destroyed and the larger vehicle on fire  .  one dead person was on the ground near the suv and a foreigner was injured he said  .  we put the injured man and the dead body in a private vehicle said mr khan  .  nothere were more injured in the surrounding area too  .  another eyewitness wajid ali said he helped put a seriously injured foreigner into the vehicle  .  but another vehicle arrived presumably from the u  . s  .  consulate and took away the injured foreigners said officer khan  .  some of the policemen escorting the u  . s  .  vehicle were also wounded in the attack and their vehicle was damaged  . \n",
      "= blast happened in north west pakistani city of peshawar today  .  two of the dead were pakistanis  .  two americans among wounded  .  u  . s  .  embassy denies initial reports that two americans were killed  . \n",
      "evaluation init\n",
      "< globocan globocan clauida 8,870 keewee liu finbarrtaylor incred 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870\n",
      "\n",
      "> by  .  leon watson  .  published  .  est march  .   .  updated  .  est march  .  a drunken reveller spent hogmanay behind bars after he was caught snorting cocaine in the back of a police car  .  unemployed salesman steven downie was arrested shortly before midnight on hogmanay after officers spotted white powder on his nose  .  the year old was clearly under the influence of alcohol and drugs and was placed in the back of a patrol vehicle  .  steven downie spent hogmanay in the cells after openly snorting coke off his hand in the back of a police car  .  then in full view of police he snorted coke from his hand  .  downie from perth in central scotland admitted possessing cocaine on december last year  .  fiscal depute robbie brown told perth sheriff court yesterday at  .  pm the officers saw the accused and they stopped to speak to him  .  he was clearly heavily under the influence of alcohol or drugs  .  they could see white powder round his nostrils  .  his behaviour became such that he was arrested  .  he was in the vehicle being taken to the police station when he was seen snorting powder from the back of his hand  .  two bags containing cocaine were found on him  .  solicitor nicky brown defending said you will see the date he had been at a party  .  he had bought this for at the party  .  pictured here with family members outside perth sheriff court downie was fined  .  the incident involving downie happened over new year in north methven street in perth central scotland  .  perth sheriff court heard steven downie was arrested shortly before midnight on hogmanay after officers spotted white powder on his nose  .  he had consumed far too much alcohol  .  when they came into town he came to the attention of police  .  she told the court he normally worked in sales but was currently unemployed  .  sheriff lindsay foulis said it is not something you want to get in the habit of taking cocaine  .  aside from anything else it can often affect your behaviour and make you become aggressive and confrontational  .  ultimately it is not cool  .  it affects your health and if you want confirmation of that you should come to this court on days when you see folk who have been taking drugs for numerous years  .  nothey may only be but they look like they are he said  .  downie was fined  . \n",
      "= steven downie arrested shortly before midnight on new year is eve  .  year old was clearly under the influence of alcohol and drugs  .  downie from perth was arrested and placed in the back of police car  .  then in full view of police he snorted coke from his hand  .  he admitted possessing cocaine and was fined  . \n",
      "evaluation init\n",
      "< clauida clauida clauida 8,870 keewee liu finbarrtaylor incred 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870\n",
      "\n",
      "> anderlecht manager besnik hasi believes arsenal were worried about entertaining their fans rather than seeing the game out after throwing away a three goal lead against the belgian side  .  the gunners left the emirates with just a point when anderlecht fought back from down in the second half to draw a result which means arsene wenger is side are not yet guaranteed to reach the knockout stages of the champions league  .  hasi who did not shake hands with wenger at full time claims the premier league side went in search of more goals in a bid to please supporters and paid for it  .  anderlecht striker aleksadar mitrovic right heads a dramatic late equaliser for his side against arsenal  .  anderlecht manager besik hasi shouts instructions to his players on the touchline at the emirates  .  mitrovic runs off to celebrate his goal in front of anderlecht is travelling supporters at the emirates  .  arsenal tried to play the same way he told the daily telegraph  .  nothey did not just sit back  .  they tried to score a fourth and at they started to have doubts  .  we could see a difference from the mental perspective  .  i think we deserved the  .  amaybe arsenal thought they were on top and they could get more goals and they could have fun in front of the public  .  at you can say it is over and they wanted the fourth and the fifth goal but we made some changes  .  with the north london side having failed to keep a clean sheet in the competition so far this season hasi insists that he always felt arsenal is defence would be vulnerable  .  alexis sanchez had given arsenal the lead but the gunners failed to win despite being up in the second half  .  hasi waves to anderlecht is travelling supporters at the emirates after leading his side to a memorable draw  .  goalscorer sanchez looks stunned after mitrovic is decisive late goal on tuesday evening  .  he said yes but to expose them you have to have confidence  .  at yes we used the spaces that they gave us  .  we have some young fast players and if they have confidence they can go  .  at you saw arsenal were doubting  .  we also changed our scheme with the second striker  .  yes arsenal doubted and we were growing  .  nothe last minutes were as intense as when we won the championship last season  .  i am a young coach but this will stay in my memory  .  we come from a small competition but we showed we can match these teams  .  over the two games against arsenal we deserve more than one point  .  video wenger rues defensive lapse  . \n",
      "= arsenal threw away a lead to draw with anderlecht on tuesday  .  the gunners failed to seal a place in the last of the champions league  .  besnik hasi believes arsene wenger is side went looking for a fourth goal  .  he also claims they thought they could have fun in front of the public \n",
      "evaluation init\n",
      "< trunck clauida clauida 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee\n",
      "\n",
      ">  mclaren is lewis hamilton scored an early boost over his three formula one world championship rivals on friday as he set the fastest time in practice ahead of sunday is abu dhabi grand prix  .  hamilton is lap time of one minute  .  seconds put him clear of red bull duo sebastian vettel and mark webber who finished second and fourth respectively with fernando alonso the favorite to take the crown at the season ending race third fastest  .  who needs to do what to win the f drivers title  .  two time champion alonso is the only one of the four who can seal the title at the yas marina circuit with a top two finish regardless of where the others finish with each of the remaining drivers needing results to go their way for victory  .  hamilton still holds a slim hope of winning his second championship he is currently fourth in the standings points behind leader alonso with points on offer for the winner of the season is final race  .  formula one standings after brazilian grand prix  .  the year old had to battle with his ferrari counterpart as they traded fast laps but it was the champion who eventually got the better of the spaniard  .  the car feels good so i am hopeful for a good qualifying result tomorrow saturday afternoon said hamilton in a mclaren statement  .  of course it will be tough because the red bulls tend to pull out half a second in the third round of qualifying but our pace is closer to theirs than it is ever been  .  of all the weekends this year this time i feel like we really do have the right package to challenge the fastest cars  .  i think we can fight for the front row  .  the first practice session was dogged by unexpected rain on the track until the skies cleared and the driver is enjoyed a disruption free second session  .  webber was almost half a second slower than the british pacesetter and knows he will need to improve over the course of the weekend if he is to win his first title  .  a repeat of the australian is fourth place in practice will require alonso to finish ninth or lower in race for him to triumph  .  it was a decent start to the weekend for year old vettel who will more than likely need maximum points from abu dhabi to avoid the disappointment of finishing runner up for the second season running  . \n",
      "= lewis hamilton set the fastest lap time in practice ahead of the abu dhabi grand prix  .  the red bull is of sebastian vettel and mark webber were second and fourth  .  championship leader fernando alonso was third fastest  . \n",
      "evaluation init\n",
      "< clauida clauida clauida 8,870 keewee liu finbarrtaylor incred 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870\n",
      "\n",
      "> charlotte and oliver are two names that will be hard to forget considering they are the frontrunners in the baby naming popularity contest  .  as the year draws to a close the two names are top of the list when it comes to baby titles in australia for  .  while more and more australian parents opt for unique names for their newborns the top ranking boy and girl names for has pretty much stayed true to the classics  .  scroll down for video  .  the top ranking boy and girl names for including oliver and charlotte have stayed true to the classics  .  charlotte olivia ava emily and mia were the top five girl names  .  top five for the boys included oliver william jack noah and jackson jaxon  .  rounding out the top was james thomas ethan lucas and cooper as well as amelia ruby sophia sofia chloe and sophie  .  many of the top names today like william jack and ruby were also high on the popularity list a century ago proving there is a hundred year return theme taking place  .  oliver emerged as australia s top boy baby name for the first time in australia s history while charlotte has been a frontrunner for several years  .  many of the top names today like william jack and ruby were also high on the popularity list a century ago proving there is a hundred year return theme taking place  .  oliver  .  william  .  jack  .  noah  .  jackson jaxon  .  james  .  thomas  .  ethan  .  lucas  .  cooper  .  charlotte  .  olivia  .  ava  .  emily  .  mia  .  amelia  .  ruby  .  sophia sofia  .  chloe  .  sophie  . \n",
      "= charlotte and oliver are top of the list for baby titles in australia for  .  top ranking boy and girl names for has stayed true to the classics  .  many of the top names today like william jack and ruby were also high on the popularity list a century ago  . \n",
      "evaluation init\n",
      "< clauida clauida clauida 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee\n",
      "\n",
      "> by  .  richard shears  .  published  .  est february  .   .  updated  .  est february  .  angry locals in a small outback town today questioned the motives of british teenager sam woodhead who set out for a desert run and then had to be rescued  .  mr woodhead was lost in the outback for three days after setting out for a late afternoon run last week and was just a few hours from dying when rescued by a helicopter crew  .  and while residents of the town of longreach are delighted that he was found and is now recovering in brisbane with his mother claire they were critical today of his motives when he set out for the ill fated run  .  life threatening sam woodhead was reunited with his mother claire derry in longreach queensland when he was rescued after spending three days missing in the australian outback  .  just a few things do not add up said norman philp who had been involved in other outback rescues  .  you get a feeling he wanted to go bush  .  mr woodhead is mother and his family in the uk have already said that the teenager is a bear grylls fan and wanted to go into the marines  .  survival expert bob cooper told brisbane s courier mail that visitors must be told that the outback is life threatening and that there are rarely any second chances  .  he said local people should try to ensure that visitors to the outback are warned of the dangers  .  we need ourselves to be more responsible in our attitude to visitors and visitors to the outback whether they re from the city or overseas he said  .  they make it into a challenge and it s more than that  .  it s not challenging it s life threatening  .  dangers mr woodhead was airlifted to hospital in longreach after being rescued from the desert  .  run that went wrong some locals in queensland suspect the bear grylls fan may have deliberately set out to test his endurance in the outback  .  hero the teenager is accused of apeing survival hero bear grylls  .  mr philp and mr cooper are among queensland residents who have questioned whether mr woodhead had deliberately set out to challenge his endurance when he left the upshot cattle station where he had been staying for the previous days  .  when he jogged off in his shorts and running shoes he was also wearing a backpack containing clothes  .  he said after his rescue that he had been using the bag to add weight to his body while he was on his jog a run he said that was just a run that went wrong  .  he has already told how he became lost and after drinking all the water from a bottle he had carried he resorted to drinking small bottles of contact lens solution he had in his backpack  .  he had also tried drinking his own urine  .  the run he had chosen he said had seemed more interesting than running around the small airfield that he had been going around on previous days  .  local resident adrian roots has joined several others in questioning mr woodhead s motives  .  from what his mum told us about him wanting to be in the royal marines and being a big fan of bear grylls that s probably why people are angry thinking that s what he was doing mr roots told the newspaper  .  there s just the unanswered questions  .  lloyd mills who is based in longreach as the general manager for outback queensland tourism said it was the responsibility of tourists to research the dangers of the outback  . \n",
      "= sam woodhead spent three days lost in australian outback  .  would be marine was close to death when he was found by rescuers  .  locals suspect he deliberately set out to test his endurance in the desert  .  the teenager has said his ordeal was result of a run that went wrong  . \n",
      "evaluation init\n",
      "< clauida clauida clauida 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee 8,870 keewee keewee\n",
      "\n",
      "> by  .  sam webb  .  published  .  est april  .   .  updated  .  est april  .  this incredible image shows two male giraffes battling it out to win a female but they look more like they are doing the tango  .  the foot tall thornicroft giraffes appear to be wouldancing with each other in what is usually a test of male dominance and mating rights  .  giraffes are considered gentle giants but often engage in bruising fights until a single giraffe is left standing  .  they can even end in death  .  the beautiful photographs shot in south luangwa national park in zambia were taken by photographer dana allen  .  dance off two male giraffes who went hoof to hoof in bid to win a female companion look more like they are dancing than engaging in deadly combat in these stunning images  .  in a giraffe fight males stand side by side pushing and shoving to judge which is the strongest  .  this pair are young bucks and are testing the other is prowess  .  photographer dana allen said because of their great size their actions seemed to be carried out in slow motion and their seemingly synchronized movements gave the impression of a well choreographed ballet he said nothey were both young bulls and they were testing each other is strength and skill  .  i watched them for nearly two hours as they tried to butt with their horns and head and tried to knock the other off balance by hooking their legs out from under them  .  because of their great size their actions seemed to be carried out in slow motion and their seemingly synchronized movements gave the impression of a well choreographed ballet  .  he added eventually the winner left with a female giraffe  .  the  .  year old american has been photographing wildlife in the southern  .  eastern and central african regions for the past years  .  dana used a nikon d s to capture the moment  .  he said nothey were pretty close up but they posed no threat to me  .  giraffes are commonly seen as gentle creatures but their courtship rites involve bruising and ferocious physical battles for dominance  .  vicious dance the pictures were shot in south luangwa national park in zambia  .  shuddering blows are sometimes exchanged using the giraffes powerful necks  .  spoils the victor of the fight won the female for breeding while the loser hobbled away to lick his wounds  .  show of force often the battles are abandoned after a short tussle but they can degenerate into a fight to the death  .  mr allen a year old american has been photographing wildlife in the southern eastern and central african regions for the past years  .  ram the horn like structures on the stop of the giraffes heads called ossicones can inflict injuries  . \n",
      "= what appears to be an elegant dance is actually a brutal fight as the two males vie for dominance  .  the extraordinary scenes were captured on camera by veteran wildlife photographer dana allen  .  giraffes are seen as placid creatures but males often engage in savage battles to get a mate  . \n",
      "evaluation init\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1127-445b3d75e531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1123-9d7626baa304>\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1122-0b07cdff2afa>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length, attn)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mdecoder_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 decoder_output, decoder_hidden = decoder(\n\u001b[0m\u001b[1;32m     31\u001b[0m                     decoder_input, decoder_hidden) \n\u001b[1;32m     32\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1094-a2cbdd011f41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#output = self.softmax(self.out(output[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# output = self.out(output[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
