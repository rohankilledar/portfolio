{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using CNN articles as dataset. These articles are stored in multiple files along with their highlights. These highlight can be used to gauge the summary output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try reading the file content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    file = open(filename, encoding= 'UTF-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "text = read_file('/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/summarisation/cnn/stories/0a0a4c90d59df9e36ffec4ba306b4f20f3ba4acb.story')\n",
    "#text.split(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the we need to do some cleaning before we start working on the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rohankilledar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# en_stopwords = stopwords.words('english')\n",
    "\n",
    "def read_file(filename):\n",
    "    file = open(filename, encoding= 'UTF-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    # cnn_indx = text.find(\"(CNN)\")\n",
    "    # if cnn_indx >=0:\n",
    "    #     text = text[cnn_indx+len(\"(CNN)\"):]\n",
    "        \n",
    "    #text = re.sub(\"\\n\",\" \",text).lower()\n",
    "\n",
    "    #text.split(\"\\n\")\n",
    "    # cleaned = list()\n",
    "    # table = str.maketrans('','',string.punctuation)\n",
    "\n",
    "    # for line in text:\n",
    "       \n",
    "    #     line = line.split(\" \")\n",
    "\n",
    "    #     line = [w.translate(table) for w in line]\n",
    "        \n",
    "    #     line = [w for w in line if w.isalpha()]\n",
    "    #     cleaned.append(' '.join(line))\n",
    "\n",
    "    # text = [sent for sent in cleaned if len(sent)>0]\n",
    "\n",
    "    return text\n",
    "\n",
    "article = read_file('/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/summarisation/cnn/stories/0a0a4c90d59df9e36ffec4ba306b4f20f3ba4acb.story')\n",
    "dmarticle = read_file('/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/summarisation/dailymail/stories/0a00a9aebcb754c51534867cf1db2335dcb76884.story')\n",
    "# t = article.split(\"\\n\")\n",
    "# cleaned = list()\n",
    "# table = str.maketrans('','',string.punctuation)\n",
    "# for w in t:\n",
    "#     if len(w)>0:\n",
    "#         w = w.split(\" \")\n",
    "#         w = [word.translate(table) for word in w]\n",
    "#         w = [word for word in w if word.isalpha()]\n",
    "#         cleaned.append(\" \".join(w))\n",
    "# print(cleaned)\n",
    "#print(dmarticle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the article and the hightlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(article):\n",
    "    indx = article.index('@highlight')\n",
    "    story = article[:indx]\n",
    "    highlight = article[indx:].split('@highlight')\n",
    "\n",
    "    highlight = \". \".join([h.strip() for h in highlight if len(h)>0])\n",
    "    return story,highlight\n",
    "\n",
    "#st,hi = split_text(dmarticle)\n",
    "#print(\"story: \"+st)\n",
    "#print(\"highlight: \"+hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st.replace(\"\\n\\n\",\" \").replace(\"\\xa0\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one of the dataset file seems to be usable now. Need to load all of the stories together using these given functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4e9f253831437ab59099af998f37d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total stories in dataset: 92579\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "def read_all(folder):\n",
    "    dataset = list()\n",
    "\n",
    "    for file in tqdm(listdir(folder)):\n",
    "        filename = folder + '/' + file\n",
    "        article = read_file(filename)\n",
    "        story,highlight = split_text(article)\n",
    "\n",
    "        dataset.append({'story':story, 'highlight':highlight})\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "folder = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/summarisation/cnn/stories\"\n",
    "daily_mail = '/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/summarisation/dailymail/stories'\n",
    "\n",
    "dataset = read_all(folder)\n",
    "# daily_mail_dataset = read_all(daily_mail)\n",
    "# dataset.extend(daily_mail_dataset)\n",
    "\n",
    "print(\"total stories in dataset: \"+ str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dataset for cleaning\n",
    "import pickle\n",
    "output_file = open('unprocessed_dataset.pkl','wb')\n",
    "pickle.dump(dataset, output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pd = pd.read_pickle('unprocessed_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pd = pd.DataFrame(dataset_pd)\n",
    "article_count = 0\n",
    "summary_count = 0\n",
    "dataset_size = len(dataset_pd)\n",
    "\n",
    "article = dataset_pd['story'].tolist()\n",
    "summary = dataset_pd['highlight'].tolist()\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    article_count += len(article[i].split())\n",
    "    summary_count += len(summary[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average article word length is: 654\n",
      "Average article word summary is: 41\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "\n",
    "print('Average article word length is: '+ str(floor(article_count/dataset_size)))\n",
    "print('Average article word summary is: '+ str(floor(summary_count/dataset_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
