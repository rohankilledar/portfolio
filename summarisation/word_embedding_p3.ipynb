{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_pickle('cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "story        0\n",
       "highlight    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pretrained word2vec\n",
    "# from gensim.test.utils import datapath\n",
    "# from gensim.models import KeyedVectors\n",
    "# path_of_downloaded_bin = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/Natural Language Processing/labs/GoogleNews-vectors-negative300.bin\"\n",
    "# # #filename = 'GoogleNews-vectors-negative300.bin'\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(datapath(path_of_downloaded_bin), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0f6affb87e2b>:10: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "path_of_downloaded_files = \"/Users/rohankilledar/Documents/MSc Artificial Intelligence/repos/Natural Language Processing/glove.6B.300d.txt\"\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(filename)\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3000000 word vectors from glove.6B.100d.txt.\n"
     ]
    }
   ],
   "source": [
    "print('loaded %s word vectors from %s.' % (len(word_vectors.key_to_index),filename ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train,y_test = train_test_split(df['story'].tolist(),df['highlight'].tolist(),test_size=0.3,random_state=2501,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64805/64805 [05:53<00:00, 183.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47561196\n",
      "215152\n"
     ]
    }
   ],
   "source": [
    "word_corpus = [] \n",
    "from nltk.tokenize import word_tokenize\n",
    "for articles in tqdm(x_train):\n",
    "    word_corpus.extend(word_tokenize(articles))\n",
    "print(len(word_corpus))\n",
    "unique_word_corpus = set(word_corpus)\n",
    "print(len(unique_word_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inter_word = set(word_vectors.key_to_index).intersection(unique_word_corpus)\n",
    "print(len(inter_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that are present in both glove vectors and our corpus are 145464 which is nearly 68.0% \n"
     ]
    }
   ],
   "source": [
    "print(\"The number of words that are present in both glove vectors and our corpus are {} which \\\n",
    "is nearly {}% \".format(len(inter_word), np.round((float(len(inter_word))/len(unique_word_corpus))\n",
    "*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knegt',\n",
       " '1401.',\n",
       " 'hink',\n",
       " 'illarionovich',\n",
       " 'whc.unesco.org',\n",
       " 'ourcq',\n",
       " 'phegley',\n",
       " 'adrb',\n",
       " 'eligendo',\n",
       " 'zhabdrung',\n",
       " 'djkokovic',\n",
       " 'refa',\n",
       " 'equivocates',\n",
       " 'knurr',\n",
       " 'klinsy',\n",
       " 'wippert',\n",
       " 'fleurival',\n",
       " 'contructor',\n",
       " 'heimans',\n",
       " 'sippie',\n",
       " 'oyefeso',\n",
       " 'ministiry',\n",
       " 'eddmenson',\n",
       " 'gurewardher',\n",
       " 'pnemonia',\n",
       " '758.4',\n",
       " 'antiscorbutics',\n",
       " 'john.smith',\n",
       " 'egality',\n",
       " 'heinekens',\n",
       " 'osobien',\n",
       " 'isteve',\n",
       " 'broncosports.com',\n",
       " 'abdelmaseeh',\n",
       " 'whbl',\n",
       " 'kawekumnerd',\n",
       " '41,363',\n",
       " 'ramineh',\n",
       " 'causes.com',\n",
       " '68,792,500',\n",
       " '62,500.',\n",
       " 'lakepowell.com',\n",
       " 'mapness',\n",
       " 'zablah',\n",
       " 'dicari',\n",
       " 'to1970s',\n",
       " 'frietas',\n",
       " 'abdelssalam',\n",
       " 'shirine',\n",
       " 'kalanick',\n",
       " 'yovanna',\n",
       " 'wouldisastrous',\n",
       " '602m',\n",
       " 'hirschy',\n",
       " 'indictator',\n",
       " 'tufegdzic',\n",
       " '16,380',\n",
       " '51f',\n",
       " 'rawker',\n",
       " 'internatioal',\n",
       " 'irva',\n",
       " 'bernerd',\n",
       " 'soei',\n",
       " 'cesbron',\n",
       " 'ruyati',\n",
       " 'sermilik',\n",
       " 'presente.org',\n",
       " 'mrporter.com',\n",
       " 'actuals',\n",
       " 'corianton',\n",
       " 'handicappedpets.com',\n",
       " 'barrando',\n",
       " 'qaracha',\n",
       " '5,202',\n",
       " '7,225',\n",
       " 'isurvivor',\n",
       " '1,700km',\n",
       " 'sevastapol',\n",
       " 'abdalaati',\n",
       " 'londolozi',\n",
       " 'gaulben',\n",
       " 'toseong',\n",
       " 'positons',\n",
       " 'drapelike',\n",
       " 'womenactmedia',\n",
       " 'chirrups',\n",
       " 'mallmann',\n",
       " '1.15pm',\n",
       " '141km',\n",
       " 'canarie',\n",
       " '19,094',\n",
       " 'flushest',\n",
       " 'tesca',\n",
       " 'u.s.financier',\n",
       " 'gruenewald',\n",
       " 'waldhauser',\n",
       " 'walsa',\n",
       " 'decriminalizes',\n",
       " 'pixelmator',\n",
       " 'edgaro',\n",
       " 'yunee',\n",
       " 'careerealism',\n",
       " 'sonit',\n",
       " 'waer',\n",
       " 'daluchie',\n",
       " 'this,103',\n",
       " 'somsuda',\n",
       " 'pathik',\n",
       " '901,935',\n",
       " 'www.mpac.jp',\n",
       " 'chaambi',\n",
       " '55.66.',\n",
       " 'kornelius',\n",
       " 'bousin',\n",
       " 'kpax',\n",
       " 'khozin',\n",
       " 'aundrea',\n",
       " 'redecorators',\n",
       " 'deterimine',\n",
       " '6014',\n",
       " 'murchinson',\n",
       " 'prachatai',\n",
       " 'duneshine',\n",
       " 'staysafe',\n",
       " 'flatteries',\n",
       " '0647',\n",
       " 'renedy',\n",
       " 'notribute',\n",
       " 'rikuzentakata',\n",
       " 'skimo',\n",
       " 'boy.i',\n",
       " '337,200',\n",
       " 'aichikanjo',\n",
       " 'avcadoctors.com',\n",
       " 'piraino',\n",
       " 'alarbaeen',\n",
       " '1,485,606',\n",
       " 'smizing',\n",
       " 'newzoo',\n",
       " 'aghai',\n",
       " 'www.uriminzokkiri.com',\n",
       " 'ownthe',\n",
       " 'musaus',\n",
       " 'lct.org',\n",
       " 'milanova',\n",
       " 'wouldefining',\n",
       " 'vahey',\n",
       " 'wouldrinking',\n",
       " 'flytethe',\n",
       " 'ivolunteer',\n",
       " 'rakela',\n",
       " 'barbaso',\n",
       " 'mirabellplatz',\n",
       " '200lr',\n",
       " 'sporyshev',\n",
       " 'guillotin',\n",
       " 'yicai',\n",
       " 'loriston',\n",
       " 'leitte',\n",
       " 'aventina',\n",
       " 'durose',\n",
       " 'benhamin',\n",
       " 'nahb.org',\n",
       " 'l.i',\n",
       " 'didavi',\n",
       " 'qeeg',\n",
       " 'modupe',\n",
       " 'rokar',\n",
       " 'kenadie',\n",
       " 'dipietros',\n",
       " 'mukusha',\n",
       " 'ukrainianise',\n",
       " 'nyantakyi',\n",
       " 'karamanoglu',\n",
       " 'michaelbrisson',\n",
       " 'moudeya',\n",
       " 'hitziges',\n",
       " 'j.balkin',\n",
       " 'khrisanfov',\n",
       " 'kamanza',\n",
       " 'douentza',\n",
       " 'gonfalon',\n",
       " 'sum1',\n",
       " 'ethylin',\n",
       " 'trami',\n",
       " 'hactivists',\n",
       " 'mohole',\n",
       " 'ictmn',\n",
       " 'pinnochios',\n",
       " 'phariss',\n",
       " 'antwuan',\n",
       " 'anotherlove',\n",
       " '25hours',\n",
       " 'datasift',\n",
       " 'boots.com',\n",
       " 'scaffidi',\n",
       " 'luchkiw',\n",
       " 'lindborg',\n",
       " 'olysio',\n",
       " '1952.',\n",
       " 'ghatt',\n",
       " 'ostebo',\n",
       " 'jarrae',\n",
       " 'zuppiger',\n",
       " 'mh371',\n",
       " '8.oakland',\n",
       " 'anthonyf84',\n",
       " '84.79',\n",
       " 'guuseum',\n",
       " 'orpah.com',\n",
       " 'ishortly',\n",
       " 'pinfluence',\n",
       " 'mogadishus',\n",
       " 'fws.gov',\n",
       " 'ghemyan',\n",
       " 'everythingkaty',\n",
       " 'hizmet',\n",
       " 'nineva',\n",
       " 'rinschler',\n",
       " 'sperminizing',\n",
       " 'bordbar',\n",
       " 'noctambules',\n",
       " 'cotage',\n",
       " 'atira',\n",
       " 'eastwooding',\n",
       " 'wouldulab',\n",
       " 'paniaha',\n",
       " 'giftcards',\n",
       " 'zuppas',\n",
       " 'amegadroughts',\n",
       " 'barkhouse',\n",
       " 'afanaseva',\n",
       " 'sofradir',\n",
       " 'odlot',\n",
       " 'bikavac',\n",
       " 'fcbarcelona.com',\n",
       " 'fartuun',\n",
       " 'vongtau',\n",
       " 'chillspot',\n",
       " 'masianai',\n",
       " 'dobinski',\n",
       " 'jounir',\n",
       " 'overusedwords',\n",
       " '2,834,797',\n",
       " 'wmbb',\n",
       " '15,047',\n",
       " 'sentiero',\n",
       " 'planeimages.net',\n",
       " '32lcx85',\n",
       " 'noogler',\n",
       " 'dreamit',\n",
       " 'whiffled',\n",
       " 'kalahariresort.com',\n",
       " 'riversong',\n",
       " 'ciarrocchi',\n",
       " 'midson',\n",
       " 'derossi',\n",
       " 'hasmat',\n",
       " 'dealio',\n",
       " 'arguido',\n",
       " 'shiine',\n",
       " 'goldzband',\n",
       " 'summeer',\n",
       " '68,802',\n",
       " 'fanshare',\n",
       " 'vystropova',\n",
       " 'levelup',\n",
       " 'jiwayli',\n",
       " '4802',\n",
       " 'guanabana',\n",
       " '60.93',\n",
       " 'q180v',\n",
       " 'folstrom',\n",
       " 'umshadi',\n",
       " 'soshanguve',\n",
       " 'maureira',\n",
       " 'divirgilio',\n",
       " 'fashioneds',\n",
       " 'radden',\n",
       " '287g',\n",
       " 'gulfrecovery',\n",
       " 'skyterrace',\n",
       " 'petropavlivka',\n",
       " 'ocwd',\n",
       " 'nischelle',\n",
       " '1927.',\n",
       " 'sisci',\n",
       " 'nsmbu',\n",
       " 'ciacci',\n",
       " 'savoreachglass.com',\n",
       " 'lowballing',\n",
       " 'lushi',\n",
       " 'sakharti',\n",
       " '69,282',\n",
       " 'sethporges',\n",
       " 'miamian',\n",
       " 'amygdalas',\n",
       " 'gogia',\n",
       " 'poulides',\n",
       " 'clayco',\n",
       " 'fehilly',\n",
       " 'stuntin',\n",
       " '2bp',\n",
       " 'www.awidercircle.org',\n",
       " 'priot',\n",
       " 'awab',\n",
       " 'gaffaney',\n",
       " 'tristin',\n",
       " 'unidentitifed',\n",
       " 'cinton',\n",
       " 'cameraperson',\n",
       " 'renen',\n",
       " 'glasshole',\n",
       " 'us4.37',\n",
       " 'mvume',\n",
       " 'kellypatricia',\n",
       " '8818',\n",
       " 'amercenarios',\n",
       " 'krindjabo',\n",
       " 'sattelite',\n",
       " 'emptily',\n",
       " 'thethemeparkguy.com',\n",
       " 'fusking',\n",
       " 'ukucula',\n",
       " 'confinity',\n",
       " 'semongkong',\n",
       " 'emfinger',\n",
       " 'kyaga',\n",
       " 'grifoni',\n",
       " 'boggus',\n",
       " 'vecome',\n",
       " 'albumatic',\n",
       " 'canevari',\n",
       " 'zhuara',\n",
       " 'kezane',\n",
       " 'casasandra',\n",
       " 'attenboroughi',\n",
       " 'ramierez',\n",
       " 'unmiss',\n",
       " 'alieva',\n",
       " 'kobusch',\n",
       " 'wedeman',\n",
       " 'flummoxing',\n",
       " 'lofving',\n",
       " '47170.',\n",
       " 'untextured',\n",
       " 'superbacteria',\n",
       " 'makison',\n",
       " 'legalizers',\n",
       " 'patricksandberg',\n",
       " 'ceelo',\n",
       " 'jelescheff',\n",
       " 'rcvfd',\n",
       " 'wippl',\n",
       " 'crough',\n",
       " 'sportspro',\n",
       " '367.6',\n",
       " 'nonhomicidal',\n",
       " 'marlantes',\n",
       " 'prolift',\n",
       " 'hooliganish',\n",
       " 'bouzar',\n",
       " '7085.',\n",
       " 'luie',\n",
       " 'tebbut',\n",
       " 'pomperaug',\n",
       " '69814',\n",
       " 'aliwalas',\n",
       " 'mroil',\n",
       " '228.56',\n",
       " 'engetsu',\n",
       " 'saaliheen',\n",
       " 'kebony',\n",
       " '133a',\n",
       " 'canidates',\n",
       " 'rpen',\n",
       " 'speechify',\n",
       " 'kikunoi',\n",
       " 'adrineh',\n",
       " 'sarvana',\n",
       " 'artprice',\n",
       " 'morbilli',\n",
       " 'donriddellcnn',\n",
       " 'qiaozi',\n",
       " 'nshamdze',\n",
       " 'trademarkable',\n",
       " 'liltunechi',\n",
       " 'www.chinalittletrips.com',\n",
       " 'chatroulette.com',\n",
       " 'kmle',\n",
       " 'civitron',\n",
       " 'sici',\n",
       " 'antekeier',\n",
       " 'dibitetto',\n",
       " '649.99',\n",
       " 'wreg',\n",
       " 'areeha',\n",
       " 'floss.com',\n",
       " 'danerous',\n",
       " 'trangendered',\n",
       " 'joejob',\n",
       " 'usian',\n",
       " '27,600.',\n",
       " 'motivazione',\n",
       " 'xvr',\n",
       " 'motorheads',\n",
       " 'amilitarization',\n",
       " 'o.co',\n",
       " '4556',\n",
       " 'danver',\n",
       " 'sqaudron',\n",
       " 'trostel',\n",
       " 'celustka',\n",
       " '2423',\n",
       " 'titong',\n",
       " 'tozaki',\n",
       " 'boehrnsen',\n",
       " 'naureen',\n",
       " '9events',\n",
       " 'peoplevobamacare.com',\n",
       " 'ciontos',\n",
       " 'machugh',\n",
       " 'thegenuineolitwist',\n",
       " 'joose',\n",
       " '38,960',\n",
       " 'shoufeng',\n",
       " 'mcnearney',\n",
       " 'menoceras',\n",
       " 'qiblawi',\n",
       " 'dumaini',\n",
       " 'www.mitchellairport.com',\n",
       " 'grame',\n",
       " 'depthless',\n",
       " 'toothwisdom.org',\n",
       " 'athest',\n",
       " '2673',\n",
       " 'gedid',\n",
       " 'husid',\n",
       " '0849',\n",
       " 'scrawlers',\n",
       " 'cop17',\n",
       " 'redbull.com',\n",
       " 'occupywallstreet',\n",
       " 'cruzando',\n",
       " 'kachroo',\n",
       " 'massiotti',\n",
       " 'kebc',\n",
       " 'jiaoyang',\n",
       " 'sholty',\n",
       " 'islave',\n",
       " 'fridel',\n",
       " 'ismoke',\n",
       " '7,149',\n",
       " '200,0000',\n",
       " 'authroties',\n",
       " 'rumeilah',\n",
       " 'felinski',\n",
       " 'mispresentation',\n",
       " 'ispace',\n",
       " '162,900',\n",
       " 'bellette',\n",
       " 'noteen',\n",
       " '207,647',\n",
       " '722.',\n",
       " 'melantrichova',\n",
       " 'amanzi.co.zw',\n",
       " 'vanderwaal',\n",
       " 'antennagate',\n",
       " 'inboxer',\n",
       " 'appreciatus',\n",
       " 'cubiclemate',\n",
       " '755m',\n",
       " 'delicates',\n",
       " 'kertz',\n",
       " 'obamacarenado',\n",
       " 'dg56789',\n",
       " 'humungous',\n",
       " 'amyotropic',\n",
       " 'slushes',\n",
       " 'jayamaha',\n",
       " 'bonza',\n",
       " 't.o.t',\n",
       " '13.65m',\n",
       " 'floralscapes',\n",
       " '6,037',\n",
       " 'bersheda',\n",
       " 'getaroom',\n",
       " 'graford',\n",
       " 'sheeha',\n",
       " 'chermoula',\n",
       " 'xlgx',\n",
       " 'sailboards',\n",
       " 'ravasio',\n",
       " '7908',\n",
       " 'lovegame',\n",
       " 'lbgt',\n",
       " 'cenric',\n",
       " 'schiavore',\n",
       " 'glassdoor.com',\n",
       " 'subtherapeutic',\n",
       " 'morawewa',\n",
       " '3323.',\n",
       " 'allahbad',\n",
       " 'calahonda',\n",
       " 'phrompan',\n",
       " 'getmethehelloutofatlanta',\n",
       " 'fulfillingness',\n",
       " 'yooks',\n",
       " 'techland.time.com',\n",
       " 'dequilettes',\n",
       " 'perigree',\n",
       " 'lelisa',\n",
       " 'andrasch',\n",
       " 'scherma',\n",
       " 'brencher',\n",
       " 'basat',\n",
       " 'refiring',\n",
       " 'mutahash',\n",
       " 'kletzky',\n",
       " 'deoman',\n",
       " 'rollocks',\n",
       " 'gaffed',\n",
       " 'slonaker',\n",
       " 'ovanesyan',\n",
       " 'baltimores',\n",
       " 'palaguachi',\n",
       " '466,368',\n",
       " 'lammerts',\n",
       " '4,952',\n",
       " 'schaibles',\n",
       " 'interweb',\n",
       " 'drader',\n",
       " 'munduk',\n",
       " 'calauit',\n",
       " 'rafio',\n",
       " 'kendro',\n",
       " 'fairdale',\n",
       " '17.3mm',\n",
       " 'ramokgwebana',\n",
       " 'bimlesh',\n",
       " 'nonaddictive',\n",
       " 'tollwood',\n",
       " 'hondecoeter',\n",
       " 'lipowitz',\n",
       " 'calvera',\n",
       " 'geiersthal',\n",
       " 'pallab',\n",
       " 'reacclimating',\n",
       " 'lapdgrimsleeper',\n",
       " '6856.',\n",
       " 'jarmichael',\n",
       " 'iamrogue.com',\n",
       " 'kushtaz',\n",
       " 'krygios',\n",
       " 'gawping',\n",
       " 'ruiner',\n",
       " 'shaff',\n",
       " 'pellicia',\n",
       " '388.',\n",
       " 'vastbarga',\n",
       " 'sackin',\n",
       " 'julaine',\n",
       " 'hasselbring',\n",
       " 'searchtrial',\n",
       " 'cyberassaults',\n",
       " '7,525',\n",
       " '12187504242',\n",
       " 'saadiya',\n",
       " 'bucheit',\n",
       " 'weichselbaum',\n",
       " 'paulwalker',\n",
       " '176x',\n",
       " 'handbananas',\n",
       " 'hemmerl',\n",
       " 'witech',\n",
       " 'eckrode',\n",
       " 'maaidah',\n",
       " 'criminales',\n",
       " 'kravarik',\n",
       " 'tecia',\n",
       " 'i.s',\n",
       " 'ickiness',\n",
       " 'vallaud',\n",
       " 'scheinkopf',\n",
       " 'surpreme',\n",
       " 'etage',\n",
       " 'hilow',\n",
       " '.38.',\n",
       " 'heath.com',\n",
       " 'luyanda',\n",
       " '10breweries',\n",
       " 'linsane',\n",
       " 'dezan',\n",
       " 'blackflix.com',\n",
       " 'weislogel',\n",
       " 'bohac',\n",
       " '4.7.',\n",
       " 'eyeborg',\n",
       " 'zerban',\n",
       " 'wagemann',\n",
       " 'ww.beijingticketing.com',\n",
       " '45.95.',\n",
       " 'sidwynicks',\n",
       " 'hidin',\n",
       " '23g',\n",
       " 'angiemartinez',\n",
       " 'adenrele',\n",
       " 'kastania',\n",
       " 'aurouze',\n",
       " 'marthie',\n",
       " 'toledos',\n",
       " 'cunningness',\n",
       " 'maihama',\n",
       " 'coffeys',\n",
       " 'minbic',\n",
       " 'nianticlabs',\n",
       " '2squar',\n",
       " 'nonremovable',\n",
       " 'occupysandy',\n",
       " 'his15',\n",
       " 'santacruzan',\n",
       " 'lauriesegallcnn',\n",
       " 'worldliest',\n",
       " 'hamidia',\n",
       " 'gerlant',\n",
       " 'inury',\n",
       " 'verlon',\n",
       " 'speedcity',\n",
       " 'qunan',\n",
       " 'www.footy',\n",
       " 'cookstove',\n",
       " 'rongfang',\n",
       " 'mygatt',\n",
       " 'dropcard',\n",
       " 'rubbernecked',\n",
       " 'alertsite',\n",
       " 'wedinger',\n",
       " 'renkl',\n",
       " 'ekaizer',\n",
       " 'enemey',\n",
       " 'macroberts',\n",
       " 'froing',\n",
       " 'oppmann',\n",
       " 'prefete',\n",
       " 'guiselman',\n",
       " 'skift',\n",
       " 'weischer',\n",
       " 'jalam',\n",
       " 'dinets',\n",
       " 'ophthalmoscopes',\n",
       " 'rodery',\n",
       " 'rolypoly',\n",
       " '298.',\n",
       " 'rumiati',\n",
       " 'loseit',\n",
       " 'thalidomiders',\n",
       " 'yapak',\n",
       " 'cincinnatifavorites',\n",
       " 'breyon',\n",
       " 'rothert',\n",
       " 'youda',\n",
       " '328,835',\n",
       " 'beautifies',\n",
       " 'suborbitals',\n",
       " 'daret',\n",
       " '35,487',\n",
       " 'exz5896',\n",
       " 'bartend',\n",
       " 'isissies',\n",
       " 'disinvitation',\n",
       " 'skycoaster',\n",
       " 'zambouros',\n",
       " '99,817',\n",
       " 'dzivarasekwa',\n",
       " 'leakin',\n",
       " 'yaton',\n",
       " 'torrres',\n",
       " 'feluccas',\n",
       " 'polyg',\n",
       " 'dragicevich',\n",
       " 'wouldiamond',\n",
       " 'chenyi',\n",
       " 'carvajales',\n",
       " 'radoncic',\n",
       " 'honeycut',\n",
       " 'cymbalsky',\n",
       " 'skyping',\n",
       " 'fadheel',\n",
       " 'enomatic',\n",
       " 'theconduct',\n",
       " 'porcellacchia',\n",
       " 'kornelyuk',\n",
       " 'olympstroy',\n",
       " '26,838',\n",
       " '1926.',\n",
       " '323,271',\n",
       " 'shebwani',\n",
       " 'nikoley',\n",
       " '7,766',\n",
       " 'dopplr',\n",
       " 'madfoot',\n",
       " 'vegasexperience.com',\n",
       " 'spatkauf',\n",
       " '4.',\n",
       " 'noontide',\n",
       " 'waterhouseshanghai.com',\n",
       " 'jtl',\n",
       " 'kmorrison',\n",
       " 'fumihito',\n",
       " 'anjelli',\n",
       " 'zeita',\n",
       " '106.',\n",
       " 'zehaf',\n",
       " 'nacuma',\n",
       " 'anarchyradio',\n",
       " 'purplepolls',\n",
       " 'roeseler',\n",
       " 'exagone',\n",
       " 'kushkush',\n",
       " '10.4mm',\n",
       " 'swaggiest',\n",
       " 'dissers',\n",
       " 'hidayah',\n",
       " 'pumhping',\n",
       " 'actorbarkhad',\n",
       " 'hutted',\n",
       " 'troyen',\n",
       " 'ascendas',\n",
       " 'diapers.com',\n",
       " '209p',\n",
       " 'secondat',\n",
       " 'mosuliya',\n",
       " 'handule',\n",
       " 'xbox360',\n",
       " 'samoy',\n",
       " 'bochud',\n",
       " 'oceaneer',\n",
       " '57m',\n",
       " 'toffoloni',\n",
       " 'kopelan',\n",
       " 'csizsik',\n",
       " 'sooie',\n",
       " 'ontarian',\n",
       " 'bereszynski',\n",
       " '163,264',\n",
       " 'cyberhate',\n",
       " 'rmb10tn',\n",
       " 'barkhat',\n",
       " 'conectd',\n",
       " 'unpol',\n",
       " 'stepinca',\n",
       " 'ruschestrasse',\n",
       " 'embassay',\n",
       " 'giannandrea',\n",
       " 'bullsh',\n",
       " '5666',\n",
       " 'tacticalgear.com',\n",
       " 'daudon',\n",
       " 'fooleywang',\n",
       " 'shrinkages',\n",
       " 'frassetta',\n",
       " '21j',\n",
       " '968,000',\n",
       " 'ted.com',\n",
       " 'xenuphiles',\n",
       " 'fiestainn.com',\n",
       " 'mbwda',\n",
       " 'gashaw',\n",
       " 'abdullrahman',\n",
       " 'gunshowgallery.com',\n",
       " 'fusioneers',\n",
       " 'outubro',\n",
       " '5,259',\n",
       " 'dickers',\n",
       " 'brandquist',\n",
       " 'sharkwater',\n",
       " 'beschizza',\n",
       " 'bellavance',\n",
       " 'laits',\n",
       " 'eshe',\n",
       " 'yeffett',\n",
       " 'r.m.s',\n",
       " 'rheumatologic',\n",
       " 'paulians',\n",
       " 'skeuomorphic',\n",
       " '1,400.',\n",
       " 'quiroa',\n",
       " 'dickheads',\n",
       " 'gmrf',\n",
       " 'bizzaro',\n",
       " 'batassa',\n",
       " 'marynell',\n",
       " 'ostrichpillow',\n",
       " '629.00',\n",
       " 'freunds',\n",
       " 'pwn2own',\n",
       " 'wrage',\n",
       " 'nonsegregated',\n",
       " '5601',\n",
       " 'enshrouds',\n",
       " 'rieur',\n",
       " 'shely',\n",
       " 'almagri',\n",
       " 'atzaret',\n",
       " 'hoefl',\n",
       " 'mauterer',\n",
       " 'wongphudee',\n",
       " 'gahaya',\n",
       " 'tsangayari',\n",
       " 'guanshengyan',\n",
       " 'davilia',\n",
       " 'parents.there',\n",
       " 'caminito',\n",
       " 'kafaschian',\n",
       " '7,015',\n",
       " 'haska',\n",
       " 'crusaderdom',\n",
       " 'bruesewitzes',\n",
       " 'constitution.net',\n",
       " 'kauppinn',\n",
       " 'gissi',\n",
       " 'albinali',\n",
       " 'mawsons',\n",
       " 'dswdserves',\n",
       " '44751',\n",
       " '2582',\n",
       " 'bukhaiti',\n",
       " 'vansyckel',\n",
       " 'mimy',\n",
       " 'issat',\n",
       " 'easytakes',\n",
       " 'ummal',\n",
       " 'squinkles',\n",
       " 'minamisanriku',\n",
       " 'adawy',\n",
       " 'obremskey',\n",
       " 'hamzas',\n",
       " 'arritu',\n",
       " 'stripteases',\n",
       " 'democracy.com',\n",
       " 'wouldisconnect',\n",
       " '255,380.',\n",
       " 'alejandouru',\n",
       " 'istars',\n",
       " 'thamal',\n",
       " 'kaylei',\n",
       " 'khedair',\n",
       " 'isims',\n",
       " 'keving',\n",
       " 'osigwe',\n",
       " 'b2300',\n",
       " 'ishallow',\n",
       " 'wintner',\n",
       " '9464.',\n",
       " 'gigayacht',\n",
       " 'fenilmarie',\n",
       " '6468',\n",
       " 'brigetta',\n",
       " 'michalewicz',\n",
       " '13,0000',\n",
       " '13.48.',\n",
       " 'joewilsonisyourpreexistingcondition',\n",
       " 'snowcaps',\n",
       " '488.68',\n",
       " 'ondoga',\n",
       " 'fanariotou',\n",
       " 'kerith',\n",
       " 'farchana',\n",
       " 'hanburger',\n",
       " 'piccos',\n",
       " 'frizzby',\n",
       " 'gosiger',\n",
       " 'obscurantisme',\n",
       " 'varnier',\n",
       " 'leighayn',\n",
       " 'lodd',\n",
       " 'terrebone',\n",
       " 'kranjak',\n",
       " '4,000m',\n",
       " 'hulkbuster',\n",
       " 'planetologie',\n",
       " 'yabaolu',\n",
       " '108bn',\n",
       " 'vars.com',\n",
       " 'ripreal',\n",
       " 'steinhotels.com',\n",
       " 'zendesk',\n",
       " '291.',\n",
       " 'shawkan',\n",
       " 'benetar',\n",
       " 'besma',\n",
       " '5919',\n",
       " 'funfetti',\n",
       " 'autonov',\n",
       " 'sophiehunter',\n",
       " 'lamri',\n",
       " 'enne',\n",
       " 'vinchuca',\n",
       " 'transtracheal',\n",
       " '11.25am',\n",
       " 'pezzano',\n",
       " 'decoufle',\n",
       " 'morlot',\n",
       " 'meteorlogical',\n",
       " 'ispeculation',\n",
       " 'qbic',\n",
       " 'eloundou',\n",
       " '8882',\n",
       " 'hpakant',\n",
       " 'wyliei',\n",
       " 'glistering',\n",
       " 'nunly',\n",
       " 'sharkapocalypse',\n",
       " 'samitar',\n",
       " 'newsgatherers',\n",
       " 'khokhe',\n",
       " 'join.not',\n",
       " 'arrt',\n",
       " 'verticale',\n",
       " 'aigbe',\n",
       " 'caelibatus',\n",
       " 'poising',\n",
       " 'syyn',\n",
       " 'no.111',\n",
       " '79.250.',\n",
       " 'arulanadam',\n",
       " 'decarbonization',\n",
       " 'kroeller',\n",
       " 'izghe',\n",
       " 'outdressed',\n",
       " 'obedia',\n",
       " 'angwan',\n",
       " 'koonz',\n",
       " 'vauxall',\n",
       " 'rosekind',\n",
       " 'buzzerbeater',\n",
       " 'hamers',\n",
       " 'karzan',\n",
       " 'nouicer',\n",
       " 'realmichellet',\n",
       " 'mirazur',\n",
       " 'levang',\n",
       " 'lactamase',\n",
       " 'branfield',\n",
       " 'ourtime.org',\n",
       " 'lasalvia',\n",
       " 'bonten',\n",
       " 'eshragi',\n",
       " 'otelli',\n",
       " '0930et',\n",
       " 'paraglided',\n",
       " 'bucciarelli',\n",
       " 'rubefacient',\n",
       " 'www.atlantis.com',\n",
       " '5720',\n",
       " '442.2',\n",
       " 'twitterjoketrial',\n",
       " 'loenne',\n",
       " 'domachowski',\n",
       " 'taquari',\n",
       " '380,000.',\n",
       " 'zimroth',\n",
       " 'rifles,18.5',\n",
       " 'adrelevance',\n",
       " 'stranged',\n",
       " 'mahankal',\n",
       " 'snickerdoodles',\n",
       " 'paradrop',\n",
       " 'wouldeception',\n",
       " 'sulaimaan',\n",
       " 'linhard',\n",
       " 'krever',\n",
       " '4,009.',\n",
       " 'hohoho',\n",
       " 'sintet',\n",
       " 'intensions',\n",
       " 'crewon',\n",
       " 'neuromotor',\n",
       " 'hilltout',\n",
       " 'catajo',\n",
       " 'orszaghaz',\n",
       " '7,969',\n",
       " 'wyspie',\n",
       " 'thetv',\n",
       " '16,270',\n",
       " 'kickings',\n",
       " 'menthane',\n",
       " 'gallaon',\n",
       " 'nonhierarchical',\n",
       " 'engluvies',\n",
       " 'humanrights',\n",
       " '26,121',\n",
       " 'marohnic',\n",
       " 'breeanna',\n",
       " 'ortellado',\n",
       " 'notaco',\n",
       " 'auerbachs',\n",
       " 'oz214',\n",
       " 'seppelt',\n",
       " '44.1m',\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_corpus.difference(set(word_vectors.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68% seems reseaonable as most of the words out of the word_vocab are either number or names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] \n",
    "word2indx = {}\n",
    "\n",
    "with open(path_of_downloaded_files) as f:\n",
    "    for indx,l in enumerate(f):\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2indx[word] = indx\n",
    "\n",
    "# prewb = len(words)\n",
    "\n",
    "# for indx,word in enumerate(unique_word_corpus):\n",
    "#     words.append(word)\n",
    "#     word2indx[word] = prewb+indx+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "215152\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vectors))\n",
    "print(len(unique_word_corpus))\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model and other necessary modules\n",
    "all_info_want_to_save = {\n",
    "    'words': words,\n",
    "    'word2indx': word2indx\n",
    "}\n",
    "\n",
    "with open(\"vocab.pkl\",\"wb\") as save_path:\n",
    "    pickle.dump(all_info_want_to_save, save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {w: word_vectors[word2indx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim =300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(unique_word_corpus)\n",
    "\n",
    "weight_matrix  = np.zeros((matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "\n",
    "for indx, word in enumerate(unique_word_corpus):\n",
    "    try:\n",
    "        weight_matrix[indx] = glove[word]\n",
    "        words_found +=1\n",
    "    except KeyError:\n",
    "        weight_matrix[indx] = np.random.normal(scale=0.6 , size= (embedding_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145464\n"
     ]
    }
   ],
   "source": [
    "print(words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weight_matrix, non_trainalbe = False):\n",
    "    num_embeddings, embedding_dim = weight_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings,embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "    if non_trainalbe:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer,num_embeddings,embedding_dim\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size,weight_matrix, bidirectional = True):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.input_size = input_size\n",
    "    self.bidirectional = bidirectional\n",
    "    self.embedding, self.num_embeddings, embedding_dim = create_emb_layer(weight_matrix,True)\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = bidirectional)\n",
    "  \n",
    "  def forward(self, inputs, hidden):\n",
    "    inputs = self.embedding(inputs)\n",
    "    output, hidden = self.lstm(inputs.view(1, 1, self.input_size), hidden)\n",
    "    return output, hidden\n",
    "    \n",
    "  def init_hidden(self):\n",
    "    return (torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size),\n",
    "      torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "  \n",
    "  def __init__(self, hidden_size, output_size, vocab_size):\n",
    "    super(AttentionDecoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    \n",
    "    self.attn = nn.Linear(hidden_size + output_size, 1)\n",
    "    self.lstm = nn.LSTM(hidden_size + vocab_size, output_size) #if we are using embedding hidden_size should be added with embedding of vocab size\n",
    "    self.final = nn.Linear(output_size, vocab_size)\n",
    "  \n",
    "  def init_hidden(self):\n",
    "    return (torch.zeros(1, 1, self.output_size),\n",
    "      torch.zeros(1, 1, self.output_size))\n",
    "  \n",
    "  def forward(self, decoder_hidden, encoder_outputs, input):\n",
    "    \n",
    "    weights = []\n",
    "    for i in range(len(encoder_outputs)):\n",
    "      print(decoder_hidden[0][0].shape)\n",
    "      print(encoder_outputs[0].shape)\n",
    "      weights.append(self.attn(torch.cat((decoder_hidden[0][0], \n",
    "                                          encoder_outputs[i]), dim = 1)))\n",
    "    normalized_weights = F.softmax(torch.cat(weights, 1), 1)\n",
    "    \n",
    "    attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\n",
    "                             encoder_outputs.view(1, -1, self.hidden_size))\n",
    "    \n",
    "    input_lstm = torch.cat((attn_applied[0], input[0]), dim = 1) #if we are using embedding, use embedding of input here instead\n",
    "    \n",
    "    output, hidden = self.lstm(input_lstm.unsqueeze(0), decoder_hidden)\n",
    "    \n",
    "    output = self.final(output[0])\n",
    "    \n",
    "    return output, hidden, normalized_weights\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Loading our Glove Model \n",
    "embeddings_index = dict()\n",
    "f = open(path_of_downloaded_files)\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 2 vec length 145464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words_corpus_source_train = {}\n",
    "words_glove = set(embeddings_index.keys())\n",
    "for i in unique_word_corpus:\n",
    "  if i in words_glove:\n",
    "    words_corpus_source_train[i] = embeddings_index[i]\n",
    "print(\"word 2 vec length\", len(words_corpus_source_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 201650\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def create_vocab(data):\n",
    "    counter = Counter()\n",
    "    for line in data:\n",
    "        counter.update(tokenizer(line))\n",
    "    vocab = Vocab(counter)\n",
    "    print('vocab len: '+ str(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "vocab = create_vocab(x_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix =np.zeros(len(unique_word_corpus)+1,embedding_dim)\n",
    "for word in unique_word_corpus:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-cae8a7ae6ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'word_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-eb9f6c0b6591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'word_index'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "x_tokenizer = sent_tokenize\n",
    "word_index = x_tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "[[-0.2852    -0.013883   0.31607   ... -0.10859   -0.14354    0.1485   ]\n",
      " [-0.23589    0.3831     0.10834   ... -0.79189   -0.08596   -0.1466   ]\n",
      " [-0.28427    0.047977  -0.15062   ... -0.090071   0.016922   0.29278  ]\n",
      " ...\n",
      " [-0.29712    0.094049  -0.096662  ...  0.059717  -0.22853    0.29602  ]\n",
      " [-0.23852   -0.33704   -0.26531   ...  0.32891    0.037239   0.23779  ]\n",
      " [ 0.080622   0.13208   -0.0047645 ... -0.063376  -0.46961    0.40538  ]]\n",
      "['how', 'are', 'you', 'doing', 'went', 'to', 'this', 'a', 'beautiful', 'place']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def vectorize_sent(word_vectors, sent):\n",
    "    word_vecs = []\n",
    "    word_list = []\n",
    "    for token in sent.split():\n",
    "        if token not in word_vectors: continue # if a word doesn't have an embedding, skip\n",
    "        \n",
    "        word_vecs.append(word_vectors[token]) \n",
    "        word_list.append(token)# otherwise, obtain the word embedding\n",
    "    # print(np.array(word_vecs).shape)\n",
    "   # return np.mean(np.array(word_vecs),axis=0) # compute the average embedding\n",
    "    return np.array(word_vecs), word_list\n",
    "\n",
    "senten = \"Hey, how are you doing today? I went to St. Andrews this weekend. It's a beautiful place\"\n",
    "print(len(senten.split()))\n",
    "sent_vec,word_list = vectorize_sent(word_vectors,senten)\n",
    "print(sent_vec)\n",
    "print(word_list)\n",
    "print(sent_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocab_file = \"data/vocab\"\n",
    "\n",
    "embedding_dim = 128\n",
    "MAX_LEN_STORY = 300\n",
    "hidden_dim = 128\n",
    "voc_size = 20000\n",
    "MAX_LEN_HIGHLIGHT = 100\n",
    "\n",
    "NUM_EPOCHS=100\n",
    "batch_size=16\n",
    "lr = 0.15\n",
    "\n",
    "## tokens variable\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "START_TOKEN = '[START]'\n",
    "STOP_TOKEN = '[STOP]'\n",
    "SENTENCE_START = '<s>' # start sentence in highlight\n",
    "SENTENCE_END = '</s>' # end sentence in highlight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1a9112920b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "loss_function = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "#from vars import *\n",
    "# from dataclasses import dataclass,field\n",
    "\n",
    "# @dataclass(frozen = True, order = True)\n",
    "# class Encoder(nn.Module):\n",
    "#     num_layer : int = 1\n",
    "#     embeddings : int = nn.Embedding(num_embeddings= voc_size, embedding_dim=hidden_dim)\n",
    "#     lstm = nn.LSTM(input_size = hidden_dim,hidden_size = hidden_dim, num_layer =1, bidrectional = True, batch_first = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layer = 1\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=voc_size,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(2 * self.num_layer, batch_size, hidden_dim, dtype=torch.float, device=device),\n",
    "                torch.zeros(2 * self.num_layer, batch_size, hidden_dim, dtype=torch.float, device=device))\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        embedded = self.embeddings(input)\n",
    "        output, (hidden, cell) = self.lstm(embedded, state)\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=voc_size,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim * 2,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim * 2,\n",
    "            out_features=voc_size\n",
    "        )\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embeddings(input).unsqueeze(1)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        output = self.fc(output.squeeze(1))\n",
    "        output = self.logsoftmax(output)\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=voc_size,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim * 2,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # attention layer (we use the definition in the paper hence tanh)\n",
    "        self.W = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_dim * 4,\n",
    "            out_features=voc_size\n",
    "        )\n",
    "        self.W_gen_sig = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + hidden_dim * 2 + hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, decoder_hidden, cell, encoder_outputs, story_extended, extra_zeros):\n",
    "        (b, t_k, n) = encoder_outputs.shape\n",
    "        embedded = self.embeddings(input).unsqueeze(1)  # batch * hidden_dim\n",
    "\n",
    "        _, (decoder_hidden, cell) = self.lstm(embedded, (decoder_hidden, cell))\n",
    "\n",
    "        decoder_hidden_expanded = decoder_hidden.permute(1, 0, 2)  # batch_size * hidden_dim\n",
    "        decoder_hidden_expanded = decoder_hidden_expanded.expand(b, t_k,\n",
    "                                                                 n).contiguous()  # batch_size * seq_len * hidden_dim\n",
    "\n",
    "        att_features = torch.cat((decoder_hidden_expanded, encoder_outputs), 2)  # batch_size * seq_lens * 2*hidden_dim\n",
    "        e_t = self.W(att_features).squeeze(2)  # batch_size * seq_lens\n",
    "        a_t = self.softmax(e_t)  # batch_size * seq_lens\n",
    "\n",
    "        a_applied = torch.bmm(a_t.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        s_t_h_t = torch.cat((decoder_hidden.squeeze(0), a_applied), 1)  # batch_size * 2*hidden_dim\n",
    "        p_vocab = self.softmax(self.fc(s_t_h_t))\n",
    "\n",
    "        cat_gen = torch.cat((a_applied, decoder_hidden.squeeze(0), embedded.squeeze(1)), 1)\n",
    "\n",
    "        p_gen = self.W_gen_sig(cat_gen).clamp(min=1e-8)\n",
    "        p_vocab = p_vocab * p_gen\n",
    "        a_t_p_gen = (1 - p_gen) * a_t\n",
    "\n",
    "        p_vocab_cat = torch.cat((p_vocab, extra_zeros), 1)\n",
    "        output = p_vocab_cat.scatter_add(1, story_extended, a_t_p_gen).clamp(min=1e-8)\n",
    "        return torch.log(output), (decoder_hidden, cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-02c3aba5c2b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "x_train_tensor = Variable(torch.Tensor(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93626/93626 [00:05<00:00, 16774.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the words in the training corpus 61270167\n",
      "unique words in the training corpus 349125\n",
      "The number of words that are present in both glove vectors and our corpus are 88176 which is nearly 25.0% \n"
     ]
    }
   ],
   "source": [
    "# Baseline model with linear fully connected network and functionality to add dropout rate\n",
    "#default activation function set to sigmoid and dropout rate to 0 making no difference to the NN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self,act_fn=nn.Sigmoid(),dropout_rate=0):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, 1000),\n",
    "        act_fn,\n",
    "        nn.Dropout(dropout_rate),\n",
    "        #nn.Sigmoid(),\n",
    "        nn.Linear(1000, 784),\n",
    "        act_fn,\n",
    "        #nn.Sigmoid(),\n",
    "        nn.Linear(784, 10)\n",
    "        \n",
    "       )\n",
    "    def forward(self, x):\n",
    "      x =  self.layers(x)\n",
    "      return x\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
